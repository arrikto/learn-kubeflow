{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Arrikto Academy! Arrikto Academy offers self-paced courses designed to help you ramp up on Kubeflow and the Kubeflow ecosystem. Courses Look for more courses in the coming weeks and months as we strive to enable MLOps and machine learning workflows with Kubeflow. Kubeflow 101 Live: Learn the Kubeflow Ecosystem and Earn Associate Certification This introductory course is for Kubeflow beginners who are brand new to the ecosystem, the tools and technologies and the vision. Kubeflow 101 On Demand: Learn the Kubeflow Ecosystem and Earn Associate Certification This introductory course is for Kubeflow beginners who are brand new to the ecosystem, the tools and technologies and the vision. Kale 101: Transform Jupyter Notebooks into Kubeflow Pipelines In this course we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models. Katib 101: Automated Hyperparameter Tuning for Models in Kubeflow Pipelines In this course we will prepare you to hyperparameter tune your models building on the fundamentals of Kale 101. Rok 101: Manage and Restore Kubeflow Pipeline Snapshots In this course we will prepare you to manage and restore your own Rok Snapshots for Kubeflow Pipelines.","title":"Home"},{"location":"#welcome-to-arrikto-academy","text":"Arrikto Academy offers self-paced courses designed to help you ramp up on Kubeflow and the Kubeflow ecosystem.","title":"Welcome to Arrikto Academy!"},{"location":"#courses","text":"Look for more courses in the coming weeks and months as we strive to enable MLOps and machine learning workflows with Kubeflow.","title":"Courses"},{"location":"#kubeflow-101-live-learn-the-kubeflow-ecosystem-and-earn-associate-certification","text":"This introductory course is for Kubeflow beginners who are brand new to the ecosystem, the tools and technologies and the vision. Kubeflow 101 On Demand: Learn the Kubeflow Ecosystem and Earn Associate Certification This introductory course is for Kubeflow beginners who are brand new to the ecosystem, the tools and technologies and the vision.","title":"Kubeflow 101 Live: Learn the Kubeflow Ecosystem and Earn Associate Certification"},{"location":"#kale-101-transform-jupyter-notebooks-into-kubeflow-pipelines","text":"In this course we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models.","title":"Kale 101: Transform Jupyter Notebooks into Kubeflow Pipelines"},{"location":"#katib-101-automated-hyperparameter-tuning-for-models-in-kubeflow-pipelines","text":"In this course we will prepare you to hyperparameter tune your models building on the fundamentals of Kale 101.","title":"Katib 101: Automated Hyperparameter Tuning for Models in Kubeflow Pipelines"},{"location":"#rok-101-manage-and-restore-kubeflow-pipeline-snapshots","text":"In this course we will prepare you to manage and restore your own Rok Snapshots for Kubeflow Pipelines.","title":"Rok 101: Manage and Restore Kubeflow Pipeline Snapshots"},{"location":"modules/license-agreement/","text":"License Agreement Copyright 2021 Arrikto Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 . Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. All usage of MiniKF is goverened by the following license agreement: https://www.arrikto.com/legal/minikf-eula/","title":"License Agreement"},{"location":"modules/license-agreement/#license-agreement","text":"Copyright 2021 Arrikto Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 . Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. All usage of MiniKF is goverened by the following license agreement: https://www.arrikto.com/legal/minikf-eula/","title":"License Agreement"},{"location":"modules/minikf-faq/","text":"MiniKF FAQ We hope this FAQ helps you answer MiniKF questions, we realize we don't have all the answers because we do not yet have all the questions! If you are not able to find an answer to your question below please reach out to us on the MiniKF channel on the Kubeflow Community slack. For quick support and resolution when you reach out please share: Which instructions are you following? Please link to them. Which step is failing? What did you expect to happen? What actually happened? (logs are tremendously helpful for the community) Index Frequently Asked Questions (FAQ) Gathering MiniKF Logs FAQ MiniKF Basics Question : What is the difference between MiniKF and regular Kubeflow installation on Kubernetes/Minikube? Answer : MiniKF is a packaged version of the whole stack that installs automatically, so that you do not have to install Kubeflow manually. We believe it is the easiest and fastest way to get up-and-running with Kubeflow. MiniKF also includes Arrikto's Rok software for advanced data management, giving you data versioning, reproducible pipelines, data protection for your notebooks, and the foundation to share workloads along with data across K8s clusters. Question : Do I need to start the VM to access MiniKF Kubeflow UI? Answer : Yes, the VM needs to be started and MiniKF needs to start up before the endpoint URL can be accessed. Question : Do I need to run minikf in order to start MiniKF in the VM? Answer : No, you can SSH into the VM and execute minikf to see status but it will autostart. Question : Does the login URL for the Kubeflow UI change each time VM is started? Answer : For GCP the URL will be static, for AWS it will change and it will be necessary to wait until the VM and MiniKF have started to see the new URL. Question : I\u2019m thinking of upgrading but noticed number 5 in the instructions says \u201cRemove all local state. This will remove all of your customization in MiniKF (notebooks, pipelines, Rok snapshots)\u201c. Is this necessary and if so, does this mean we need to manually export notebooks, datasets, pipelines, etc if we want to preserve our work? Answer : Unfortunately, we don't support MiniKF upgrades without losing the existing data. However, we do work with customers to provide enterprise Kubeflow support and/or support for their MiniKF deployments. If this describes your situation please reach out to your account manager. Question : I am unable to start MiniKF due to account not validated error. Answer : Before launching MiniKF in either GCP or AWS you must first validate your account with the cloud provider. MiniKF & Tutorials Question : I tried to run the tutorials from https://www.arrikto.com/tutorials/ and I am running into missing package errors. Answer : You must make sure the packages are imported by running the first cell or any import commands and then restarting the notebook kernel. As an example consider reviewing our Kale 101 Course - Get Dataset and Code section MiniKF & Vagrant Question : I am experiencing massive memory issues when running the Titanic example on my laptop (MiniKF on Vagrant). Answer : The Titanic example requires higher memory than the MiniKF VM on Vagrant has, you could upgrade your VM's RAM, but make sure that the host has enough RAM to support this. The easiest way is to run the example using MiniKF on GCP: https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : I started MiniKF on a personal / private server and would like to share it on the network. What's the proper way of doing it? I tried forwarding port with Vagrant: config.vm.network \"forwarded_port\", guest: 80, host: 8081 # MiniKF Terminal config.vm.network \"forwarded_port\", guest: 8080, host: 8080 # KubeFlow config.vm.network \"forwarded_port\", guest: 7681, host: 7681 # Although MiniKF Terminal is available, Kubeflow UI is not responding. Answer : Add in guest_ip: \"10.10.10.10\" in the above lines config.vm.network \"forwarded_port\", guest: 80, host: 8081 # MiniKF config.vm.network \"forwarded_port\", guest: 8080, guest_ip: \"10.10.10.10\", host: 8080 # kubectl config.vm.network \"forwarded_port\", guest: 7681, host: 7681 # Question : I cannot install MiniKF on my laptop. Answer : Your system must been the following requirements. https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/#system-requirements Question : I have a MacBook that I believe meets the requirements, but I cannot install MiniKF. Answer : While we test MiniKF on various MacBook models we can't test it exhaustively on all of them. With that said in general consider: Monitoring your CPU utilization and check if your Macbook overheats when starting MiniKF. Consider using a faster machine, if you have one available. Try MiniKF on GCP. https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : Can I expose the MiniKF UI to be accessed from outside the host? Answer : You will need to forward the necessary ports to the host, and possibly create some firewall rules. See the relevant Vagrant docs: https://www.vagrantup.com/docs/networking/forwarded_ports.html Question : On my machine, I have 128G memory and 28 CPUs. By default, the Minikube installation of the Vagrant Box uses 2 CPUs and 10G of memory. How can I increase this to 8 CPUs and 32G of memory. I amended the VM specs from within the VirtualBox UI. Answer : Although you can edit your VM from within the VirtualBox UI, you shouldn't, because Vagrant will overwrite [at least some of] your changes based on what is in the Vagrantfile. So, it's best to edit the Vagrantfile and specify the CPU and RAM you need. Question : During install Vagrant hangs at pod 33 / 34. Answer : This is likely due to a memory issue, make sure that the Vagrantfile is providing 24 GB (okay) - 32 (best) GB of Ram for Kubelow install. Question : How do I access MiniKF behind a proxy? Answer : To access MiniKF behind a proxy you can use the vagrant-proxyconf plugin and set Vagrantfile accordingly after vagrant init . This link provides more info regarding proxy configuration for Vagrant. Question : I cannot install MiniKF. I get the following error message: VT-x is not available (VERR_VMX_NO_VMX) Answer : Hyper-V disables VT-x for other hypervisors, so VirtualBox cannot use it. Hyper-V disables VT-x for other hypervisors and VirtualBox says there is not VT-x available because when hyper-V is installed on Windows, the hypervisor is running all the time underneath the host OS. Only one process can control the VT hardware at a time for stability. The hypervisor blocks all other calls to the VT hardware. To proceed, you need to disable Hyper-V: Open CMD as administrator. Turn off Hyper-V by running: bcdedit /set hypervisorlaunchtype off . Reboot. To turn it back on: Run bcdedit /set hypervisorlaunchtype . Reboot. Question : After completing Vagrant install unable to open http://10.10.10.10/ due to timeout. Answer : Destroy and restart the VM to resolve the issue. Question : While following the Vagrant install steps here https://v0-6.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/ Vagrant returns an error The IP address configured for the host-only network is not within the allowed ranges. Answer : This is a Vagrant issue and results from using a version after 6.1.26 (https://discourse.roots.io/t/the-ip-address-configured-for-the-host-only-network-is-not-within-the-allowed-ranges/21679/3). To correct this please use version 6.1.26 and make sure you are following the most recent directions: https://v0-6.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/ MiniKF & GCP Question : When I try to get MiniKF to run on GCP, I get resource level errors. Answer : You cannot run MiniKF (or Kubeflow in general) using the GCP Free Tier. See the official docs: https://www.kubeflow.org/docs/gke/deploy/project-setup/ https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : When I try to get MiniKF to run on GCP, I get an QUOTA_EXCEEDED error. Answer : For default values, MiniKF requires 200G for boot disk and 500G for data disk. You will need to select a region with a greater quota limit. Please request a quota increase for your account if none are available. MiniKF & Windows Question : If i have to install Kubeflow on Windows should I install MiniKF only or is there is something else I can install? Answer : You should only install MiniKF assuming you already have Vagrant and VirtualBox on your windows machine. See the requirements here: https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/ MiniKF & MacOS Question : I\u2019d like to try MiniKF on macos, but I don\u2019t want to install VirtualBox. How can I get it working with HyperKit or Docker? Answer : VirtualBox is a prerequisite for using MiniKF on your laptop. As an alternative, you can use MiniKF on GCP: https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ MiniKF & Notebooks Question : When I do a PIP install, it says permission denied. When I open a terminal through Jupyter notebook and run sudo pip install, it asks for a password for the account jovyan. Answer : Please run this command from inside a Notebook Server: pip3 install --user <lirary> MiniKF Logs Collecting MiniKF logs depends on where MiniKF is being used. Vagrant For Vagrant, in general, to collect and review logs you must: vagrant ssh into your MiniKF VM. run minikf-gather-logs . This will produce a tarball .tgz file in your MiniKF directory. Open the tarball file to see logs. When necessary and instructed share the tarball file with Arrikto experts. GCP For GCP, in general, to collect and review logs you must: SSH from the Browser window . run minikf-gather-logs . This will produce a tarball .tar.bz2 file under /vagrant/ . Download the file as described in this guide . The exact path to download the logs file is /vagrant/minikf-logs-<date>-<time>.tar.bz2 . Open the tarball file to see logs. When necessary and instructed share the tarball file with Arrikto experts.","title":"MiniKF FAQ"},{"location":"modules/minikf-faq/#minikf-faq","text":"We hope this FAQ helps you answer MiniKF questions, we realize we don't have all the answers because we do not yet have all the questions! If you are not able to find an answer to your question below please reach out to us on the MiniKF channel on the Kubeflow Community slack. For quick support and resolution when you reach out please share: Which instructions are you following? Please link to them. Which step is failing? What did you expect to happen? What actually happened? (logs are tremendously helpful for the community)","title":"MiniKF FAQ"},{"location":"modules/minikf-faq/#index","text":"Frequently Asked Questions (FAQ) Gathering MiniKF Logs","title":"Index"},{"location":"modules/minikf-faq/#faq","text":"","title":"FAQ"},{"location":"modules/minikf-faq/#minikf-basics","text":"Question : What is the difference between MiniKF and regular Kubeflow installation on Kubernetes/Minikube? Answer : MiniKF is a packaged version of the whole stack that installs automatically, so that you do not have to install Kubeflow manually. We believe it is the easiest and fastest way to get up-and-running with Kubeflow. MiniKF also includes Arrikto's Rok software for advanced data management, giving you data versioning, reproducible pipelines, data protection for your notebooks, and the foundation to share workloads along with data across K8s clusters. Question : Do I need to start the VM to access MiniKF Kubeflow UI? Answer : Yes, the VM needs to be started and MiniKF needs to start up before the endpoint URL can be accessed. Question : Do I need to run minikf in order to start MiniKF in the VM? Answer : No, you can SSH into the VM and execute minikf to see status but it will autostart. Question : Does the login URL for the Kubeflow UI change each time VM is started? Answer : For GCP the URL will be static, for AWS it will change and it will be necessary to wait until the VM and MiniKF have started to see the new URL. Question : I\u2019m thinking of upgrading but noticed number 5 in the instructions says \u201cRemove all local state. This will remove all of your customization in MiniKF (notebooks, pipelines, Rok snapshots)\u201c. Is this necessary and if so, does this mean we need to manually export notebooks, datasets, pipelines, etc if we want to preserve our work? Answer : Unfortunately, we don't support MiniKF upgrades without losing the existing data. However, we do work with customers to provide enterprise Kubeflow support and/or support for their MiniKF deployments. If this describes your situation please reach out to your account manager. Question : I am unable to start MiniKF due to account not validated error. Answer : Before launching MiniKF in either GCP or AWS you must first validate your account with the cloud provider.","title":"MiniKF Basics"},{"location":"modules/minikf-faq/#minikf-tutorials","text":"Question : I tried to run the tutorials from https://www.arrikto.com/tutorials/ and I am running into missing package errors. Answer : You must make sure the packages are imported by running the first cell or any import commands and then restarting the notebook kernel. As an example consider reviewing our Kale 101 Course - Get Dataset and Code section","title":"MiniKF &amp; Tutorials"},{"location":"modules/minikf-faq/#minikf-vagrant","text":"Question : I am experiencing massive memory issues when running the Titanic example on my laptop (MiniKF on Vagrant). Answer : The Titanic example requires higher memory than the MiniKF VM on Vagrant has, you could upgrade your VM's RAM, but make sure that the host has enough RAM to support this. The easiest way is to run the example using MiniKF on GCP: https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : I started MiniKF on a personal / private server and would like to share it on the network. What's the proper way of doing it? I tried forwarding port with Vagrant: config.vm.network \"forwarded_port\", guest: 80, host: 8081 # MiniKF Terminal config.vm.network \"forwarded_port\", guest: 8080, host: 8080 # KubeFlow config.vm.network \"forwarded_port\", guest: 7681, host: 7681 # Although MiniKF Terminal is available, Kubeflow UI is not responding. Answer : Add in guest_ip: \"10.10.10.10\" in the above lines config.vm.network \"forwarded_port\", guest: 80, host: 8081 # MiniKF config.vm.network \"forwarded_port\", guest: 8080, guest_ip: \"10.10.10.10\", host: 8080 # kubectl config.vm.network \"forwarded_port\", guest: 7681, host: 7681 # Question : I cannot install MiniKF on my laptop. Answer : Your system must been the following requirements. https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/#system-requirements Question : I have a MacBook that I believe meets the requirements, but I cannot install MiniKF. Answer : While we test MiniKF on various MacBook models we can't test it exhaustively on all of them. With that said in general consider: Monitoring your CPU utilization and check if your Macbook overheats when starting MiniKF. Consider using a faster machine, if you have one available. Try MiniKF on GCP. https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : Can I expose the MiniKF UI to be accessed from outside the host? Answer : You will need to forward the necessary ports to the host, and possibly create some firewall rules. See the relevant Vagrant docs: https://www.vagrantup.com/docs/networking/forwarded_ports.html Question : On my machine, I have 128G memory and 28 CPUs. By default, the Minikube installation of the Vagrant Box uses 2 CPUs and 10G of memory. How can I increase this to 8 CPUs and 32G of memory. I amended the VM specs from within the VirtualBox UI. Answer : Although you can edit your VM from within the VirtualBox UI, you shouldn't, because Vagrant will overwrite [at least some of] your changes based on what is in the Vagrantfile. So, it's best to edit the Vagrantfile and specify the CPU and RAM you need. Question : During install Vagrant hangs at pod 33 / 34. Answer : This is likely due to a memory issue, make sure that the Vagrantfile is providing 24 GB (okay) - 32 (best) GB of Ram for Kubelow install. Question : How do I access MiniKF behind a proxy? Answer : To access MiniKF behind a proxy you can use the vagrant-proxyconf plugin and set Vagrantfile accordingly after vagrant init . This link provides more info regarding proxy configuration for Vagrant. Question : I cannot install MiniKF. I get the following error message: VT-x is not available (VERR_VMX_NO_VMX) Answer : Hyper-V disables VT-x for other hypervisors, so VirtualBox cannot use it. Hyper-V disables VT-x for other hypervisors and VirtualBox says there is not VT-x available because when hyper-V is installed on Windows, the hypervisor is running all the time underneath the host OS. Only one process can control the VT hardware at a time for stability. The hypervisor blocks all other calls to the VT hardware. To proceed, you need to disable Hyper-V: Open CMD as administrator. Turn off Hyper-V by running: bcdedit /set hypervisorlaunchtype off . Reboot. To turn it back on: Run bcdedit /set hypervisorlaunchtype . Reboot. Question : After completing Vagrant install unable to open http://10.10.10.10/ due to timeout. Answer : Destroy and restart the VM to resolve the issue. Question : While following the Vagrant install steps here https://v0-6.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/ Vagrant returns an error The IP address configured for the host-only network is not within the allowed ranges. Answer : This is a Vagrant issue and results from using a version after 6.1.26 (https://discourse.roots.io/t/the-ip-address-configured-for-the-host-only-network-is-not-within-the-allowed-ranges/21679/3). To correct this please use version 6.1.26 and make sure you are following the most recent directions: https://v0-6.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/","title":"MiniKF &amp; Vagrant"},{"location":"modules/minikf-faq/#minikf-gcp","text":"Question : When I try to get MiniKF to run on GCP, I get resource level errors. Answer : You cannot run MiniKF (or Kubeflow in general) using the GCP Free Tier. See the official docs: https://www.kubeflow.org/docs/gke/deploy/project-setup/ https://www.kubeflow.org/docs/started/workstation/minikf-gcp/ Question : When I try to get MiniKF to run on GCP, I get an QUOTA_EXCEEDED error. Answer : For default values, MiniKF requires 200G for boot disk and 500G for data disk. You will need to select a region with a greater quota limit. Please request a quota increase for your account if none are available.","title":"MiniKF &amp; GCP"},{"location":"modules/minikf-faq/#minikf-windows","text":"Question : If i have to install Kubeflow on Windows should I install MiniKF only or is there is something else I can install? Answer : You should only install MiniKF assuming you already have Vagrant and VirtualBox on your windows machine. See the requirements here: https://www.kubeflow.org/docs/other-guides/virtual-dev/getting-started-minikf/","title":"MiniKF &amp; Windows"},{"location":"modules/minikf-faq/#minikf-macos","text":"Question : I\u2019d like to try MiniKF on macos, but I don\u2019t want to install VirtualBox. How can I get it working with HyperKit or Docker? Answer : VirtualBox is a prerequisite for using MiniKF on your laptop. As an alternative, you can use MiniKF on GCP: https://www.kubeflow.org/docs/started/workstation/minikf-gcp/","title":"MiniKF &amp; MacOS"},{"location":"modules/minikf-faq/#minikf-notebooks","text":"Question : When I do a PIP install, it says permission denied. When I open a terminal through Jupyter notebook and run sudo pip install, it asks for a password for the account jovyan. Answer : Please run this command from inside a Notebook Server: pip3 install --user <lirary>","title":"MiniKF &amp; Notebooks"},{"location":"modules/minikf-faq/#minikf-logs","text":"Collecting MiniKF logs depends on where MiniKF is being used.","title":"MiniKF Logs"},{"location":"modules/minikf-faq/#vagrant","text":"For Vagrant, in general, to collect and review logs you must: vagrant ssh into your MiniKF VM. run minikf-gather-logs . This will produce a tarball .tgz file in your MiniKF directory. Open the tarball file to see logs. When necessary and instructed share the tarball file with Arrikto experts.","title":"Vagrant"},{"location":"modules/minikf-faq/#gcp","text":"For GCP, in general, to collect and review logs you must: SSH from the Browser window . run minikf-gather-logs . This will produce a tarball .tar.bz2 file under /vagrant/ . Download the file as described in this guide . The exact path to download the logs file is /vagrant/minikf-logs-<date>-<time>.tar.bz2 . Open the tarball file to see logs. When necessary and instructed share the tarball file with Arrikto experts.","title":"GCP"},{"location":"modules/deploy/minikf-gcp/","text":"Set Up MiniKF on Google Cloud Two Options You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions. Follow the Set Up Video Follow the Step-by-Step Instructions 1. Find MiniKF in the Google Cloud Marketplace. Open Google Cloud Marketplace and search for \"MiniKF\". 2. Select the MiniKF virtual machine by Arrikto. 3. Click the LAUNCH button and select your project. 4. Choose a name and zone for your MiniKF instance. In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button. 5. Wait for the MiniKF compute instance to boot up. 6. SSH to MiniKF When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete. 7. Log in to MiniKF When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"Deploy MiniKF on GCP"},{"location":"modules/deploy/minikf-gcp/#set-up-minikf-on-google-cloud","text":"","title":"Set Up MiniKF on Google Cloud"},{"location":"modules/deploy/minikf-gcp/#two-options","text":"You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions.","title":"Two Options"},{"location":"modules/deploy/minikf-gcp/#follow-the-set-up-video","text":"","title":"Follow the Set Up Video"},{"location":"modules/deploy/minikf-gcp/#follow-the-step-by-step-instructions","text":"","title":"Follow the Step-by-Step Instructions"},{"location":"modules/deploy/minikf-gcp/#1-find-minikf-in-the-google-cloud-marketplace","text":"Open Google Cloud Marketplace and search for \"MiniKF\".","title":"1. Find MiniKF in the Google Cloud Marketplace."},{"location":"modules/deploy/minikf-gcp/#2-select-the-minikf-virtual-machine-by-arrikto","text":"","title":"2. Select the MiniKF virtual machine by Arrikto."},{"location":"modules/deploy/minikf-gcp/#3-click-the-launch-button-and-select-your-project","text":"","title":"3. Click the LAUNCH button and select your project."},{"location":"modules/deploy/minikf-gcp/#4-choose-a-name-and-zone-for-your-minikf-instance","text":"In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button.","title":"4. Choose a name and zone for your MiniKF instance."},{"location":"modules/deploy/minikf-gcp/#5-wait-for-the-minikf-compute-instance-to-boot-up","text":"","title":"5. Wait for the MiniKF compute instance to boot up."},{"location":"modules/deploy/minikf-gcp/#6-ssh-to-minikf","text":"When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete.","title":"6. SSH to MiniKF"},{"location":"modules/deploy/minikf-gcp/#7-log-in-to-minikf","text":"When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"7. Log in to MiniKF"},{"location":"modules/images/","text":"","title":"Index"},{"location":"modules/instructor-led/","text":"Arrikto 101 : Instructor Led Labs Prepare MiniKF To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below. 1. View the Home screen in your MiniKF Kubeflow deployment. 2. Select the Notebooks pane from the main navigation menu. You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty. 3. Click the NEW SERVER button. Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server. 4. Enter a name. In the Name field enter arrikto101-lab 5. Add a data volume. Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you. 6. Click the LAUNCH button. Scroll to the bottom of the form and click the LAUNCH button to create your notebook server. 7. Connect to your notebook server. To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab. Environment Preparation Now that you have a Notebook Server set up you need to download and add the relevant handout files to the Notebook Server. 1. Download and Unzip the handout files Download and unzip the handout . car_prices.csv is your data file. data_dictionary-carprices.xlsx provides some explanatory detail on your dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on your dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for your notebook. We'll use this file to install those requirements in a later step. 2. Open the arrikto101-lab folder Double-click on the directory, arrikto101-lab . 3. Click the file upload button 4. Upload handout files In the file dialog that pops up, select the handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the arrikto101-lab directory. 5. Create a new folder Click the button to create a new folder. 6. Name the folder \"data\" 7. Move data files Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder. 8. Open your notebook Double-click predict_car_price.ipynb in the file browser pane. 9. Enable Kale Click the Enable toggle in the Kale Deployment panel to enable Kale. 10. Launch a Terminal Click the Launcher tab and launch a terminal. 11. Install Requirements In the new terminal enter cd arrikto101-lab-vol-1 Now that you are in the right directory install the Python modules required by this notebook. pip install -r requirements.txt 12. Restart the Kernel Return to the predict_car_price.ipynb notebook, restart the kernel. Lab Approach As you proceed from here with the lab you will be given both outcomes to work towards as well as steps on how to work towards these outcomes. Not all outcomes have steps, this is by design to help you build skills in this area. If you are unsure how to proceed please ask your instructor or refer to the presentation for supplemental information. Creating Pipeline w/ Kale Now that you are set up you will practice tagging cells with Kale to generate and automatically launch a pipeline in Kubeflow. You will first set up your initial pipeline steps - prep_data and clean_data . 1. Isolate the code for your step in one cell Modify your code so that the line that reads the car_prices.csv file is in a cell by itself. Once you are complete your cells should resemble the below: 2. Annotate the cell as a Pipeline Step and name it Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of your pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, your entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment. Let\u2019s define the second step of your pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly. 3. Annotate the Clean Data step Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following. In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. The step clean_data relies on read_data to read your dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute. 4. Organize Pipeline Modelling Steps Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. Scroll down to find them, they are as follows. Move these imports so that they are together with the others in the first cell of your notebook. Once you've done that, the first cell in your notebook will look like this. The modified cells in the Visualize Data section should look like this. Now, let\u2019s apply the Imports annotation to the cell containing your reorganized imports. Your Imports cell should look like this when completed. Now you need to do the same for the Modeling section of your notebook. Scroll to the Modeling section and make the following changes: Create a cell just below the Modeling head. Move all import statements from below this point in the notebook into the new cell. Note that there are imports in several places, make sure you collect them all. Annotate the new cell you created as an Import cell. When you are done the cell should look like this. 5. Skip Unnecessary read_data and clean_data Cells In their current form, your read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in your notebook where it will be of use as we continue to develop your models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below. Update the cells in the Clean Data section of your notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. This includes the cells that contain: df_auto.dtypes df_auto.loc[df_auto.duplicated()] df_auto.head() 6. Skip Visualization Sections The Visualize Data section of this notebook informs how we build models. We don\u2019t need to run the cells in the Visualize Data section as part of your pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine your models further and want to return to a visualization and analysis phase in a future iteration. To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined. Update your copy of your notebook so that all cells in the Visualize Data section of your notebook will be excluded from the Kubeflow pipeline we are building. 7. Organize prep_data Step In the Prep Data section of your notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for your regression models and only including these columns in your data frame. Encoding categorical variables so that they can be used by your models. To incorporate the Prep Data section as a step in your Kubeflow pipeline, modify your copy of your notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct Depends On as clean_data . Include all cells that contain code that is core to the step prep_data . When you have completed the above your cell should look like this. Additionally you will need to exclude cells in the Prep Data section that are not core to the functionality of this step. These include: df_auto.head() corr=df_auto.corr() corr.style.background_gradient(cmap=\"inferno\") plt.figure(figsize=(15,15)) sns.heatmap(df_auto.corr(), annot=True, cmap='inferno', mask=np.triu(df_auto.corr(),k=1)) 8. Organize split_data Step In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of your workflow. Annotate the cell in the Build Models section of your notebook and call it split_data and have it depend on prep_data step. When complete your cell should look like this. 9. Finish Designing Pipeline To complete your pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It does not make sense to combine the model training and evaluation code in a single cell or step, which is how the notebook is currently configured on the back of investigation. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. The code for the LGBM model is depicted in the figure above. In the notebook this is split into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of your pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Your pipeline can now be depicted as: 10. Create train_rf and eval_rf steps Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in your pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. When complete the train_rf , eval_rf and skip cells should look like this. Now do the same for the XGB model, when complete your cells should look like this. 11. Run Your Pipeline Now it\u2019s time to run your pipeline. In the Kale Deployment panel, which you opened by selecting Enable Click the pull-down menu for Select experiment and create a new experiment called arrikto101-lab. Enter \u201carrikto-lab\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for your models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of your models. Feel free to explore other output tabs and other aspects of the pipeline run. If Pipeline Errors Out The most likely cause of this, besides incorrect tagging, is not having installed the packages from the requirements.txt file. Please make sure that you revisit this step - Environment Preparation - Install Requirements if you error out here. Hyperparameter Tuning w/ Katib Now that you have confirmed that your Kale tagging is working and creating pipelines it is time to perform hyperparameter tuning. Typically you would look a the results of the eval_model steps to identify the ideal model to tune. For the sake of this lab we will focus on XGB. 1. Imports Cell Create a new cell directly below the first Imports cell. Create three new hyperparameters with the respective default values. NUM_ESTIMATORS = 500 MAX_DEPTH = 3 LEARNING_RATE = 0.01 Click the edit icon and tag the cell with Pipeline Parameters . Scroll down to the cell where the xgb model is created and edit the cell to parametize the XGB model creation. xgb1 = XGBRegressor() parameters = {'n_estimators': [NUM_ESTIMATORS], 'max_depth': [MAX_DEPTH], 'learning_rate': [LEARNING_RATE]} xgb_grid = GridSearchCV(xgb1,parameters,cv = 2) xgb_grid.fit(x,y) When completed your cell should look like this. In order to perform an experiment within Kale you will need to move at least one print(eval_metric) statements into the final cell of the notebook. You will also need to tag this cell with Pipeline Metrics . Create a new cell at the bottom of the Notebook and enter print(xgb_r2_score) Tag the cell with pipeline-metrics so Kale recognizes this cell when creating the Katib Job. When completed your cell should look like this. 2. Skip Cells You will need to omit all unnecessary steps from your pipeline given that at this point you know the model you want to work with. Within the Build Models section scroll down to the train_lgbm cell. Edit this cell and change the Kale tag to skip . Still in the Build Models section scroll down to the eval_lgbm cell. Edit this cell and change the Kale tag to skip . Perform the same for the train_rf and eval_rf cells. 3. Set Up Katib Job The preparation done in advance ensures that when you go to configure a Katib Job all the options will be prepopulated or will be based on what has been defined in the notebook cells. Toggle the HP Tuning with Katib option to set up the Katib Job. To configure the hyperparameter tuning job you will need to configure each of the following in the Set Up Katib Job UI. NUM_ESTIMATORS should start at 400 and go to 600 at intervals of 10. MAX_DEPTH should be between 1 and 5 at intervals of 1. LEARNING_RATE should be between 0 and 1 at intervals of 0.1. Additionally: The Search Algorithm should be Grid Search. The Search Objective should be maximizing the value of r squared. You should do at last 3 trials, at most 12 and stop after 3 failed attempts. When completed your Katib Job should look like this. Once you have configured the job click Compile and Run to begin execution. 4. Interpret Katib Output Once execution has completed, which may take a while, you identify the ideal hyperparameters by reviewing the output from either the Experiments (KFP) UI. Navigate to the Experiments(AutoML) UI to confirm that the Katib Job is being executed. Once the experiments have completed you will be able to select this entry to view the results. The UI will show a graph of attempted values and scrolling down will show the optimized values. Your output will look like this. Snapshotting w/ Rok Throughout your work Rok has been taking snapshots for you to rapidly restore Notebooks or individual steps in the Kubeflow pipeline. Now you will load one of these Snapshots into a new Notebook Server. 1. Open Snapshots In the Kubeflow UI select Snapshots to view the Rok buckets that have been created. 2. Open Bucket Expanding the arrikto101-lab bucket will show the list of snapshots that have been taken during the lab. In the Snapshots UI select the bucket for the lab. 3. Copy URL for Snapshot In the Kubeflow UI create a new Notebook Server and copy the URL from the top Rok Snapshot. 4. Paste into Notebook Server Create a new Notebook Server and name it arrikto101-lab-rok . Paste the Rok URL into the Rok URL option. You will see a notification on screen once the snapshot has been recognized. Once this is done select 'LAUNCH' 5. Open Notebook Server Once the Notebook Server has been created open it to confirm that the Notebook is loaded and available. Congratulations! You have successfully completed the Arrikto 101 Instructor Led Lab. Thank you for your attention and dedication to learning. If you have subsequent questions please find us on slack or reach out to alex.aidun@arrikto.com","title":"**Arrikto 101**: Instructor Led Labs"},{"location":"modules/instructor-led/#arrikto-101-instructor-led-labs","text":"","title":"Arrikto 101: Instructor Led Labs"},{"location":"modules/instructor-led/#prepare-minikf","text":"To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below.","title":"Prepare MiniKF"},{"location":"modules/instructor-led/#1-view-the-home-screen-in-your-minikf-kubeflow-deployment","text":"","title":"1. View the Home screen in your MiniKF Kubeflow deployment."},{"location":"modules/instructor-led/#2-select-the-notebooks-pane-from-the-main-navigation-menu","text":"You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty.","title":"2. Select the Notebooks pane from the main navigation menu."},{"location":"modules/instructor-led/#3-click-the-new-server-button","text":"Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server.","title":"3. Click the NEW SERVER button."},{"location":"modules/instructor-led/#4-enter-a-name","text":"In the Name field enter arrikto101-lab","title":"4. Enter a name."},{"location":"modules/instructor-led/#5-add-a-data-volume","text":"Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you.","title":"5. Add a data volume."},{"location":"modules/instructor-led/#6-click-the-launch-button","text":"Scroll to the bottom of the form and click the LAUNCH button to create your notebook server.","title":"6. Click the LAUNCH button."},{"location":"modules/instructor-led/#7-connect-to-your-notebook-server","text":"To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"7. Connect to your notebook server."},{"location":"modules/instructor-led/#environment-preparation","text":"Now that you have a Notebook Server set up you need to download and add the relevant handout files to the Notebook Server.","title":"Environment Preparation"},{"location":"modules/instructor-led/#1-download-and-unzip-the-handout-files","text":"Download and unzip the handout . car_prices.csv is your data file. data_dictionary-carprices.xlsx provides some explanatory detail on your dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on your dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for your notebook. We'll use this file to install those requirements in a later step.","title":"1. Download and Unzip the handout files"},{"location":"modules/instructor-led/#2-open-the-arrikto101-lab-folder","text":"Double-click on the directory, arrikto101-lab .","title":"2. Open the arrikto101-lab folder"},{"location":"modules/instructor-led/#3-click-the-file-upload-button","text":"","title":"3. Click the file upload button"},{"location":"modules/instructor-led/#4-upload-handout-files","text":"In the file dialog that pops up, select the handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the arrikto101-lab directory.","title":"4. Upload handout files"},{"location":"modules/instructor-led/#5-create-a-new-folder","text":"Click the button to create a new folder.","title":"5. Create a new folder"},{"location":"modules/instructor-led/#6-name-the-folder-data","text":"","title":"6. Name the folder \"data\""},{"location":"modules/instructor-led/#7-move-data-files","text":"Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder.","title":"7. Move data files"},{"location":"modules/instructor-led/#8-open-your-notebook","text":"Double-click predict_car_price.ipynb in the file browser pane.","title":"8. Open your notebook"},{"location":"modules/instructor-led/#9-enable-kale","text":"Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"9. Enable Kale"},{"location":"modules/instructor-led/#10-launch-a-terminal","text":"Click the Launcher tab and launch a terminal.","title":"10. Launch a Terminal"},{"location":"modules/instructor-led/#11-install-requirements","text":"In the new terminal enter cd arrikto101-lab-vol-1 Now that you are in the right directory install the Python modules required by this notebook. pip install -r requirements.txt","title":"11. Install Requirements"},{"location":"modules/instructor-led/#12-restart-the-kernel","text":"Return to the predict_car_price.ipynb notebook, restart the kernel. Lab Approach As you proceed from here with the lab you will be given both outcomes to work towards as well as steps on how to work towards these outcomes. Not all outcomes have steps, this is by design to help you build skills in this area. If you are unsure how to proceed please ask your instructor or refer to the presentation for supplemental information.","title":"12. Restart the Kernel"},{"location":"modules/instructor-led/#creating-pipeline-w-kale","text":"Now that you are set up you will practice tagging cells with Kale to generate and automatically launch a pipeline in Kubeflow. You will first set up your initial pipeline steps - prep_data and clean_data .","title":"Creating Pipeline w/ Kale"},{"location":"modules/instructor-led/#1-isolate-the-code-for-your-step-in-one-cell","text":"Modify your code so that the line that reads the car_prices.csv file is in a cell by itself. Once you are complete your cells should resemble the below:","title":"1. Isolate the code for your step in one cell"},{"location":"modules/instructor-led/#2-annotate-the-cell-as-a-pipeline-step-and-name-it","text":"Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of your pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, your entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment. Let\u2019s define the second step of your pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly.","title":"2. Annotate the cell as a Pipeline Step and name it"},{"location":"modules/instructor-led/#3-annotate-the-clean-data-step","text":"Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following. In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. The step clean_data relies on read_data to read your dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute.","title":"3. Annotate the Clean Data step"},{"location":"modules/instructor-led/#4-organize-pipeline-modelling-steps","text":"Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. Scroll down to find them, they are as follows. Move these imports so that they are together with the others in the first cell of your notebook. Once you've done that, the first cell in your notebook will look like this. The modified cells in the Visualize Data section should look like this. Now, let\u2019s apply the Imports annotation to the cell containing your reorganized imports. Your Imports cell should look like this when completed. Now you need to do the same for the Modeling section of your notebook. Scroll to the Modeling section and make the following changes: Create a cell just below the Modeling head. Move all import statements from below this point in the notebook into the new cell. Note that there are imports in several places, make sure you collect them all. Annotate the new cell you created as an Import cell. When you are done the cell should look like this.","title":"4. Organize Pipeline Modelling Steps"},{"location":"modules/instructor-led/#5-skip-unnecessary-read_data-and-clean_data-cells","text":"In their current form, your read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in your notebook where it will be of use as we continue to develop your models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below. Update the cells in the Clean Data section of your notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. This includes the cells that contain: df_auto.dtypes df_auto.loc[df_auto.duplicated()] df_auto.head()","title":"5. Skip Unnecessary read_data and clean_data Cells"},{"location":"modules/instructor-led/#6-skip-visualization-sections","text":"The Visualize Data section of this notebook informs how we build models. We don\u2019t need to run the cells in the Visualize Data section as part of your pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine your models further and want to return to a visualization and analysis phase in a future iteration. To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined. Update your copy of your notebook so that all cells in the Visualize Data section of your notebook will be excluded from the Kubeflow pipeline we are building.","title":"6. Skip Visualization Sections"},{"location":"modules/instructor-led/#7-organize-prep_data-step","text":"In the Prep Data section of your notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for your regression models and only including these columns in your data frame. Encoding categorical variables so that they can be used by your models. To incorporate the Prep Data section as a step in your Kubeflow pipeline, modify your copy of your notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct Depends On as clean_data . Include all cells that contain code that is core to the step prep_data . When you have completed the above your cell should look like this. Additionally you will need to exclude cells in the Prep Data section that are not core to the functionality of this step. These include: df_auto.head() corr=df_auto.corr() corr.style.background_gradient(cmap=\"inferno\") plt.figure(figsize=(15,15)) sns.heatmap(df_auto.corr(), annot=True, cmap='inferno', mask=np.triu(df_auto.corr(),k=1))","title":"7. Organize prep_data Step"},{"location":"modules/instructor-led/#8-organize-split_data-step","text":"In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of your workflow. Annotate the cell in the Build Models section of your notebook and call it split_data and have it depend on prep_data step. When complete your cell should look like this.","title":"8. Organize split_data Step"},{"location":"modules/instructor-led/#9-finish-designing-pipeline","text":"To complete your pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It does not make sense to combine the model training and evaluation code in a single cell or step, which is how the notebook is currently configured on the back of investigation. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. The code for the LGBM model is depicted in the figure above. In the notebook this is split into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of your pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Your pipeline can now be depicted as:","title":"9. Finish Designing Pipeline"},{"location":"modules/instructor-led/#10-create-train_rf-and-eval_rf-steps","text":"Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in your pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. When complete the train_rf , eval_rf and skip cells should look like this. Now do the same for the XGB model, when complete your cells should look like this.","title":"10. Create train_rf and eval_rf steps"},{"location":"modules/instructor-led/#11-run-your-pipeline","text":"Now it\u2019s time to run your pipeline. In the Kale Deployment panel, which you opened by selecting Enable Click the pull-down menu for Select experiment and create a new experiment called arrikto101-lab. Enter \u201carrikto-lab\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for your models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of your models. Feel free to explore other output tabs and other aspects of the pipeline run. If Pipeline Errors Out The most likely cause of this, besides incorrect tagging, is not having installed the packages from the requirements.txt file. Please make sure that you revisit this step - Environment Preparation - Install Requirements if you error out here.","title":"11. Run Your Pipeline"},{"location":"modules/instructor-led/#hyperparameter-tuning-w-katib","text":"Now that you have confirmed that your Kale tagging is working and creating pipelines it is time to perform hyperparameter tuning. Typically you would look a the results of the eval_model steps to identify the ideal model to tune. For the sake of this lab we will focus on XGB.","title":"Hyperparameter Tuning w/ Katib"},{"location":"modules/instructor-led/#1-imports-cell","text":"Create a new cell directly below the first Imports cell. Create three new hyperparameters with the respective default values. NUM_ESTIMATORS = 500 MAX_DEPTH = 3 LEARNING_RATE = 0.01 Click the edit icon and tag the cell with Pipeline Parameters . Scroll down to the cell where the xgb model is created and edit the cell to parametize the XGB model creation. xgb1 = XGBRegressor() parameters = {'n_estimators': [NUM_ESTIMATORS], 'max_depth': [MAX_DEPTH], 'learning_rate': [LEARNING_RATE]} xgb_grid = GridSearchCV(xgb1,parameters,cv = 2) xgb_grid.fit(x,y) When completed your cell should look like this. In order to perform an experiment within Kale you will need to move at least one print(eval_metric) statements into the final cell of the notebook. You will also need to tag this cell with Pipeline Metrics . Create a new cell at the bottom of the Notebook and enter print(xgb_r2_score) Tag the cell with pipeline-metrics so Kale recognizes this cell when creating the Katib Job. When completed your cell should look like this.","title":"1. Imports Cell"},{"location":"modules/instructor-led/#2-skip-cells","text":"You will need to omit all unnecessary steps from your pipeline given that at this point you know the model you want to work with. Within the Build Models section scroll down to the train_lgbm cell. Edit this cell and change the Kale tag to skip . Still in the Build Models section scroll down to the eval_lgbm cell. Edit this cell and change the Kale tag to skip . Perform the same for the train_rf and eval_rf cells.","title":"2. Skip Cells"},{"location":"modules/instructor-led/#3-set-up-katib-job","text":"The preparation done in advance ensures that when you go to configure a Katib Job all the options will be prepopulated or will be based on what has been defined in the notebook cells. Toggle the HP Tuning with Katib option to set up the Katib Job. To configure the hyperparameter tuning job you will need to configure each of the following in the Set Up Katib Job UI. NUM_ESTIMATORS should start at 400 and go to 600 at intervals of 10. MAX_DEPTH should be between 1 and 5 at intervals of 1. LEARNING_RATE should be between 0 and 1 at intervals of 0.1. Additionally: The Search Algorithm should be Grid Search. The Search Objective should be maximizing the value of r squared. You should do at last 3 trials, at most 12 and stop after 3 failed attempts. When completed your Katib Job should look like this. Once you have configured the job click Compile and Run to begin execution.","title":"3. Set Up Katib Job"},{"location":"modules/instructor-led/#4-interpret-katib-output","text":"Once execution has completed, which may take a while, you identify the ideal hyperparameters by reviewing the output from either the Experiments (KFP) UI. Navigate to the Experiments(AutoML) UI to confirm that the Katib Job is being executed. Once the experiments have completed you will be able to select this entry to view the results. The UI will show a graph of attempted values and scrolling down will show the optimized values. Your output will look like this.","title":"4. Interpret Katib Output"},{"location":"modules/instructor-led/#snapshotting-w-rok","text":"Throughout your work Rok has been taking snapshots for you to rapidly restore Notebooks or individual steps in the Kubeflow pipeline. Now you will load one of these Snapshots into a new Notebook Server.","title":"Snapshotting w/ Rok"},{"location":"modules/instructor-led/#1-open-snapshots","text":"In the Kubeflow UI select Snapshots to view the Rok buckets that have been created.","title":"1. Open Snapshots"},{"location":"modules/instructor-led/#2-open-bucket","text":"Expanding the arrikto101-lab bucket will show the list of snapshots that have been taken during the lab. In the Snapshots UI select the bucket for the lab.","title":"2. Open Bucket"},{"location":"modules/instructor-led/#3-copy-url-for-snapshot","text":"In the Kubeflow UI create a new Notebook Server and copy the URL from the top Rok Snapshot.","title":"3. Copy URL for Snapshot"},{"location":"modules/instructor-led/#4-paste-into-notebook-server","text":"Create a new Notebook Server and name it arrikto101-lab-rok . Paste the Rok URL into the Rok URL option. You will see a notification on screen once the snapshot has been recognized. Once this is done select 'LAUNCH'","title":"4. Paste into Notebook Server"},{"location":"modules/instructor-led/#5-open-notebook-server","text":"Once the Notebook Server has been created open it to confirm that the Notebook is loaded and available.","title":"5. Open Notebook Server"},{"location":"modules/instructor-led/#congratulations","text":"You have successfully completed the Arrikto 101 Instructor Led Lab. Thank you for your attention and dedication to learning. If you have subsequent questions please find us on slack or reach out to alex.aidun@arrikto.com","title":"Congratulations!"},{"location":"modules/notebook-katib-tuning/","text":"Katib 101 : HyperParameter Tuning via Jupyter Notebook, Kale & Katib Course Summary Working with Katib allows you to perform hyperparameter tuning to improve models built with Kale in Jupyter Notebooks. In this module, we will prepare you to define Katib experiments using Kubeflow pipelines, run multiple experiments in parallel and and interpret the results to identify the ideal model. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We will teach you how to build on this functionality to define and run multiple Katib experiments. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension. We are assuming that you know how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. If you need a refresher on these skills, please review our Kale 101 course","title":"Home"},{"location":"modules/notebook-katib-tuning/#katib-101-hyperparameter-tuning-via-jupyter-notebook-kale-katib","text":"","title":"Katib 101:  HyperParameter Tuning via Jupyter Notebook, Kale &amp; Katib"},{"location":"modules/notebook-katib-tuning/#course-summary","text":"Working with Katib allows you to perform hyperparameter tuning to improve models built with Kale in Jupyter Notebooks. In this module, we will prepare you to define Katib experiments using Kubeflow pipelines, run multiple experiments in parallel and and interpret the results to identify the ideal model. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We will teach you how to build on this functionality to define and run multiple Katib experiments. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension. We are assuming that you know how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. If you need a refresher on these skills, please review our Kale 101 course","title":"Course Summary"},{"location":"modules/notebook-katib-tuning/hyperparameter-overview/","text":"Hyperparameter Tuning Hyperparameters Hyperparameters are variables that control the model training process. You can use hyperparameter tuning to maximize the predictive accuracy of your models. Hyperparameter Tuning Automated hyperparameter tuning optimizes a target objective that you specify. A common metric is the model\u2019s accuracy in the validation pass of the training job (validation-accuracy) or the model quality metric you are focused on. You can specify whether you want the hyperparameter tuning job to maximize or minimize the metric. Katib will run several trials. Each trial tests a different set of values for your hyperparameters. At the end of the experiment, Katib outputs the optimized values for the hyperparameters. Post Hyperparameter Tuning Once you have produced the ideal set of hyperparameters you are prepared to serve the most ideal version of your model into production.","title":"Hyperparameter Tuning Overview"},{"location":"modules/notebook-katib-tuning/hyperparameter-overview/#hyperparameter-tuning","text":"","title":"Hyperparameter Tuning"},{"location":"modules/notebook-katib-tuning/hyperparameter-overview/#hyperparameters","text":"Hyperparameters are variables that control the model training process. You can use hyperparameter tuning to maximize the predictive accuracy of your models.","title":"Hyperparameters"},{"location":"modules/notebook-katib-tuning/hyperparameter-overview/#hyperparameter-tuning_1","text":"Automated hyperparameter tuning optimizes a target objective that you specify. A common metric is the model\u2019s accuracy in the validation pass of the training job (validation-accuracy) or the model quality metric you are focused on. You can specify whether you want the hyperparameter tuning job to maximize or minimize the metric. Katib will run several trials. Each trial tests a different set of values for your hyperparameters. At the end of the experiment, Katib outputs the optimized values for the hyperparameters.","title":"Hyperparameter Tuning"},{"location":"modules/notebook-katib-tuning/hyperparameter-overview/#post-hyperparameter-tuning","text":"Once you have produced the ideal set of hyperparameters you are prepared to serve the most ideal version of your model into production.","title":"Post Hyperparameter Tuning"},{"location":"modules/notebook-katib-tuning/katib-101-overview/","text":"Course Overview","title":"Course Overview"},{"location":"modules/notebook-katib-tuning/katib-101-overview/#course-overview","text":"","title":"Course Overview"},{"location":"modules/notebook-katib-tuning/katib-job/","text":"Katib Job Hyperparameter tuning attempts to optimize the set of hyperparameters for the defined pipeline metric. You will repeatedly run experiment trials while observing the impact to the pipeline metric to identify the ideal trial and as such ideal model. When configuring a Katib Job there are several parameters that you will need to set. Enable Katib Follow Along Please follow along in your own Notebook for the subsequent steps. 1. Toggle Enable Katib Toggle Enable Katib from the Kale UI. 2. Set Up Katib Job Click Set Up Katib Job to see the Katib Job configuration. Before proceeding we will pause to review the Katib Job configuration options. Search Space Parameters The Search Space Parameters are the set of all possible hyperparameter values that the Katib trials will adjust while attempting to optimize the Search Objective. Note that Katib will automatically pick up the Search Space Parameters from the cell tagged Pipeline Parameters . For each hyperparameter, you may provide a minimum and maximum value or a list of allowable values. Katib will test different values for the Search Space Parameters using this range or list of values during experimentation to optimize for the best model. Search Algorithm The Search Algorithm is the technique Katib will use to optimize the hyperparameter values for the desired optimization of the Search Objective. Katib in MiniKF or EKF offers several Search Algorithms for you to select from based on your need. For a detailed review of the available options please read our documentation. Search Objective The Search Objective is the metric you want the hyperparameter tuning to optimize. Note that this is picked up from the Pipeline Metrics cell tagged via Kale. You also specify whether you want the hyperparameter tuning job to maximize or minimize the metric. The default way to calculate the experiment\u2019s objective is: When the objective type is maximize, Katib compares all maximum metric values. When the objective type is minimize, Katib compares all minimum metric values. Run Parameters The Run Parameters put boundaries on the trials and Katib will run the experiment until the corresponding successful trials reach maxTrialCount. parallelTrialCount: The maximum number of hyperparameter sets that Katib should train in parallel. The default value is 3. maxTrialCount: The maximum number of trials to run. This is equivalent to the number of hyperparameter sets that Katib should generate to test the model. If the maxTrialCount value is omitted, your experiment will be running until the objective goal is reached or the experiment reaches a maximum number of failed trials. maxFailedTrialCount: The maximum number of failed trials before Katib should stop the experiment. This is equivalent to the number of failed hyperparameter sets that Katib should test. If the number of failed trials exceeds maxFailedTrialCount, Katib stops the experiment with a status of Failed. Advanced Settings In addition to standard set up the Advanced Settings allows you to configure a few additional parameters. You may select the docker image. You may enable debugability. You may elect to use the volumes directly attached to the Notebook. You may elect to have Rok take snapshots during each step.","title":"Katib Job"},{"location":"modules/notebook-katib-tuning/katib-job/#katib-job","text":"Hyperparameter tuning attempts to optimize the set of hyperparameters for the defined pipeline metric. You will repeatedly run experiment trials while observing the impact to the pipeline metric to identify the ideal trial and as such ideal model. When configuring a Katib Job there are several parameters that you will need to set.","title":"Katib Job"},{"location":"modules/notebook-katib-tuning/katib-job/#enable-katib","text":"Follow Along Please follow along in your own Notebook for the subsequent steps.","title":"Enable Katib"},{"location":"modules/notebook-katib-tuning/katib-job/#1-toggle-enable-katib","text":"Toggle Enable Katib from the Kale UI.","title":"1. Toggle Enable Katib"},{"location":"modules/notebook-katib-tuning/katib-job/#2-set-up-katib-job","text":"Click Set Up Katib Job to see the Katib Job configuration. Before proceeding we will pause to review the Katib Job configuration options.","title":"2. Set Up Katib Job"},{"location":"modules/notebook-katib-tuning/katib-job/#search-space-parameters","text":"The Search Space Parameters are the set of all possible hyperparameter values that the Katib trials will adjust while attempting to optimize the Search Objective. Note that Katib will automatically pick up the Search Space Parameters from the cell tagged Pipeline Parameters . For each hyperparameter, you may provide a minimum and maximum value or a list of allowable values. Katib will test different values for the Search Space Parameters using this range or list of values during experimentation to optimize for the best model.","title":"Search Space Parameters"},{"location":"modules/notebook-katib-tuning/katib-job/#search-algorithm","text":"The Search Algorithm is the technique Katib will use to optimize the hyperparameter values for the desired optimization of the Search Objective. Katib in MiniKF or EKF offers several Search Algorithms for you to select from based on your need. For a detailed review of the available options please read our documentation.","title":"Search Algorithm"},{"location":"modules/notebook-katib-tuning/katib-job/#search-objective","text":"The Search Objective is the metric you want the hyperparameter tuning to optimize. Note that this is picked up from the Pipeline Metrics cell tagged via Kale. You also specify whether you want the hyperparameter tuning job to maximize or minimize the metric. The default way to calculate the experiment\u2019s objective is: When the objective type is maximize, Katib compares all maximum metric values. When the objective type is minimize, Katib compares all minimum metric values.","title":"Search Objective"},{"location":"modules/notebook-katib-tuning/katib-job/#run-parameters","text":"The Run Parameters put boundaries on the trials and Katib will run the experiment until the corresponding successful trials reach maxTrialCount. parallelTrialCount: The maximum number of hyperparameter sets that Katib should train in parallel. The default value is 3. maxTrialCount: The maximum number of trials to run. This is equivalent to the number of hyperparameter sets that Katib should generate to test the model. If the maxTrialCount value is omitted, your experiment will be running until the objective goal is reached or the experiment reaches a maximum number of failed trials. maxFailedTrialCount: The maximum number of failed trials before Katib should stop the experiment. This is equivalent to the number of failed hyperparameter sets that Katib should test. If the number of failed trials exceeds maxFailedTrialCount, Katib stops the experiment with a status of Failed.","title":"Run Parameters"},{"location":"modules/notebook-katib-tuning/katib-job/#advanced-settings","text":"In addition to standard set up the Advanced Settings allows you to configure a few additional parameters. You may select the docker image. You may enable debugability. You may elect to use the volumes directly attached to the Notebook. You may elect to have Rok take snapshots during each step.","title":"Advanced Settings"},{"location":"modules/notebook-katib-tuning/katib-output/","text":"Katib Output Katib Execution During execution Katib will run an experiment to identify the best hyperparameter values as per the objective and associated function (maximize or minimize) specified when setting up the Katib job. The output of each of this experiment as well as the performance details and the overall best run are all available via the Kubeflow UI. Since Katib is being facilitated by Kale you can confirm the pipeline created by clicking VIEW. Experiments (KFP) The Experiments (KFP) page shows the experiments that have been run in KubeFlow pipelines with the names based on the Kale and Katib inputs from the JupyterLabs Notebook. Selecting an individual experiment will provide a list of the individual runs that comprise the experiments, keeping in mind that this number is limited based on values provided in the Run Parameters section. The page also displays at the end of each run the value achieved for the Search Objective. Experiments (AutoML) The Experiments (AutoML) page shows the list of experiments. Selecting an expirement will show greater detail about the trial runs and the associated hyperparameters that Katib is attempting to optimize. The page opens to a graph showing the experiments and the hyperparameter values considered. Scrolling down shows the number of runs and the breakdown of successful and failed runs as well as the achieved value of the optimal Search Objective. Ideal Model The ideal model hyperparameters based on the Katib tuning using the prescribed Search Algorithm and the associated Search Metrics are available through either Experiments UI. As a good practice we recommend taking the optimized hyperparameters and using them in the production models that you will serve based on the pipelines that are built with Kale & Katib.","title":"Katib Output"},{"location":"modules/notebook-katib-tuning/katib-output/#katib-output","text":"","title":"Katib Output"},{"location":"modules/notebook-katib-tuning/katib-output/#katib-execution","text":"During execution Katib will run an experiment to identify the best hyperparameter values as per the objective and associated function (maximize or minimize) specified when setting up the Katib job. The output of each of this experiment as well as the performance details and the overall best run are all available via the Kubeflow UI. Since Katib is being facilitated by Kale you can confirm the pipeline created by clicking VIEW.","title":"Katib Execution"},{"location":"modules/notebook-katib-tuning/katib-output/#experiments-kfp","text":"The Experiments (KFP) page shows the experiments that have been run in KubeFlow pipelines with the names based on the Kale and Katib inputs from the JupyterLabs Notebook. Selecting an individual experiment will provide a list of the individual runs that comprise the experiments, keeping in mind that this number is limited based on values provided in the Run Parameters section. The page also displays at the end of each run the value achieved for the Search Objective.","title":"Experiments (KFP)"},{"location":"modules/notebook-katib-tuning/katib-output/#experiments-automl","text":"The Experiments (AutoML) page shows the list of experiments. Selecting an expirement will show greater detail about the trial runs and the associated hyperparameters that Katib is attempting to optimize. The page opens to a graph showing the experiments and the hyperparameter values considered. Scrolling down shows the number of runs and the breakdown of successful and failed runs as well as the achieved value of the optimal Search Objective.","title":"Experiments (AutoML)"},{"location":"modules/notebook-katib-tuning/katib-output/#ideal-model","text":"The ideal model hyperparameters based on the Katib tuning using the prescribed Search Algorithm and the associated Search Metrics are available through either Experiments UI. As a good practice we recommend taking the optimized hyperparameters and using them in the production models that you will serve based on the pipelines that are built with Kale & Katib.","title":"Ideal Model"},{"location":"modules/notebook-katib-tuning/katib-tuning-next-steps/","text":"Katib Hyperparameter Tuning Summary In this course you have learned to do the following: Evaluate multiple models within a notebook to identify the best model for hyperparamter tuning. Prepare a Jupyter Notebook for hyperparameter tuning by isolating and tagging Pipeline Metrics . Create an execute a Katib Job and review output to identify the ideal hyperparameters. Next Steps After identifying your ideal model and ideal hyperparameter you are prepared to finalize and serve your model. This can be done with the KFServing and the SDK from within the notebook - for an example of this please see the public tutorial From Notebook to Kubeflow Pipelines to KFServing: the Data Science Odyssey Additional Education Please consider taking our additional courses to expand your continuing Kubeflow education!","title":"Next Steps"},{"location":"modules/notebook-katib-tuning/katib-tuning-next-steps/#katib-hyperparameter-tuning","text":"","title":"Katib Hyperparameter Tuning"},{"location":"modules/notebook-katib-tuning/katib-tuning-next-steps/#summary","text":"In this course you have learned to do the following: Evaluate multiple models within a notebook to identify the best model for hyperparamter tuning. Prepare a Jupyter Notebook for hyperparameter tuning by isolating and tagging Pipeline Metrics . Create an execute a Katib Job and review output to identify the ideal hyperparameters.","title":"Summary"},{"location":"modules/notebook-katib-tuning/katib-tuning-next-steps/#next-steps","text":"After identifying your ideal model and ideal hyperparameter you are prepared to finalize and serve your model. This can be done with the KFServing and the SDK from within the notebook - for an example of this please see the public tutorial From Notebook to Kubeflow Pipelines to KFServing: the Data Science Odyssey","title":"Next Steps"},{"location":"modules/notebook-katib-tuning/katib-tuning-next-steps/#additional-education","text":"Please consider taking our additional courses to expand your continuing Kubeflow education!","title":"Additional Education"},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/","text":"Launch a Notebook Server from a Base Image To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below. 1. View the Home screen in your MiniKF Kubeflow deployment. 2. Select the Notebooks pane from the main navigation menu. You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty. 3. Click the NEW SERVER button. Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server. 4. Enter a name. In the Name field, enter a name, e.g., learn-katib-pipelines. 5. Add a data volume. Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you. 6. Click the LAUNCH button. Scroll to the bottom of the form and click the LAUNCH button to create your notebook server. 7. Connect to your notebook server. To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"Deploy a Notebook Server"},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#launch-a-notebook-server-from-a-base-image","text":"To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below.","title":"Launch a Notebook Server from a Base Image"},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#1-view-the-home-screen-in-your-minikf-kubeflow-deployment","text":"","title":"1. View the Home screen in your MiniKF Kubeflow deployment."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#2-select-the-notebooks-pane-from-the-main-navigation-menu","text":"You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty.","title":"2. Select the Notebooks pane from the main navigation menu."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#3-click-the-new-server-button","text":"Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server.","title":"3. Click the NEW SERVER button."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#4-enter-a-name","text":"In the Name field, enter a name, e.g., learn-katib-pipelines.","title":"4. Enter a name."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#5-add-a-data-volume","text":"Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you.","title":"5. Add a data volume."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#6-click-the-launch-button","text":"Scroll to the bottom of the form and click the LAUNCH button to create your notebook server.","title":"6. Click the LAUNCH button."},{"location":"modules/notebook-katib-tuning/notebook-server-base-image/#7-connect-to-your-notebook-server","text":"To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"7. Connect to your notebook server."},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/","text":"Preparing for Hyperparameter Tuning - Hyperparameters & Objectives Once you have identified the best performing model you will need to identify the set of hyperparameters that you would like to tune. You will also need to identify an objective metric to use as the basis for evaluating the relative performance of different hyperparameter settings. Lastly, you will want to exclude models we are no longer considering from the pipeline by skipping all cells for those models. To perform hyperparameter tuning all parameters must first be introduced as variables and not as static inputs as parameters in the model function call. Secondly for Kale and Katib to consider the hyperparameters for tuning they must be in their own cell and tagged accordingly. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. 1. Create Hyperparamter Cell Create a new cell directly below the first Imports cell. Create three new hyperparameters with the respective default values. NUM_ESTIMATORS = 500 MAX_DEPTH = 3 LEARNING_RATE = 0.01 Click the edit icon and tag the cell with Pipeline Parameters . 2. Parametize XGB Model Scroll down to the cell where the xgb model is created and edit the cell to parametize the XGB model creation. xgb1 = XGBRegressor() parameters = {'n_estimators': [NUM_ESTIMATORS], 'max_depth': [MAX_DEPTH], 'learning_rate': [LEARNING_RATE]} xgb_grid = GridSearchCV(xgb1,parameters,cv = 2) xgb_grid.fit(x,y) 3. Set Pipeline Metrics for Optimization In order to perform an experiment within Kale you will need to add print(eval_metric) statements into the final cell of the notebook. You will also need to tag this cell with Pipeline Metrics . Create a new cell at the bottom of the Notebook and enter print(xgb_r2_score) You will complete this cell as part of the next lab. 4. Skip Unnecessary Cells You will need to omit all unnecessary steps from your pipeline given that at this point you know the model you want to work with. Skipping these cells will reduce the size of the KubeFlow pipeline that is created for experimentation and subsequently speed up the hyperparameter tuning process. Within the Build Models section scroll down to the train_lgbm cell. Edit this cell and change the Kale tag to skip . Still in the Build Models section scroll down to the eval_lgbm cell. Edit this cell and change the Kale tag to skip . You will complete skipping cells are part of the next lab.","title":"Preparing Hyperparameter Tuning Hyperparameters & Objectives"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/#preparing-for-hyperparameter-tuning-hyperparameters-objectives","text":"Once you have identified the best performing model you will need to identify the set of hyperparameters that you would like to tune. You will also need to identify an objective metric to use as the basis for evaluating the relative performance of different hyperparameter settings. Lastly, you will want to exclude models we are no longer considering from the pipeline by skipping all cells for those models. To perform hyperparameter tuning all parameters must first be introduced as variables and not as static inputs as parameters in the model function call. Secondly for Kale and Katib to consider the hyperparameters for tuning they must be in their own cell and tagged accordingly. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Preparing for Hyperparameter Tuning - Hyperparameters &amp; Objectives"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/#1-create-hyperparamter-cell","text":"Create a new cell directly below the first Imports cell. Create three new hyperparameters with the respective default values. NUM_ESTIMATORS = 500 MAX_DEPTH = 3 LEARNING_RATE = 0.01 Click the edit icon and tag the cell with Pipeline Parameters .","title":"1. Create Hyperparamter Cell"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/#2-parametize-xgb-model","text":"Scroll down to the cell where the xgb model is created and edit the cell to parametize the XGB model creation. xgb1 = XGBRegressor() parameters = {'n_estimators': [NUM_ESTIMATORS], 'max_depth': [MAX_DEPTH], 'learning_rate': [LEARNING_RATE]} xgb_grid = GridSearchCV(xgb1,parameters,cv = 2) xgb_grid.fit(x,y)","title":"2. Parametize XGB Model"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/#3-set-pipeline-metrics-for-optimization","text":"In order to perform an experiment within Kale you will need to add print(eval_metric) statements into the final cell of the notebook. You will also need to tag this cell with Pipeline Metrics . Create a new cell at the bottom of the Notebook and enter print(xgb_r2_score) You will complete this cell as part of the next lab.","title":"3. Set Pipeline Metrics for Optimization"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning-parttwo/#4-skip-unnecessary-cells","text":"You will need to omit all unnecessary steps from your pipeline given that at this point you know the model you want to work with. Skipping these cells will reduce the size of the KubeFlow pipeline that is created for experimentation and subsequently speed up the hyperparameter tuning process. Within the Build Models section scroll down to the train_lgbm cell. Edit this cell and change the Kale tag to skip . Still in the Build Models section scroll down to the eval_lgbm cell. Edit this cell and change the Kale tag to skip . You will complete skipping cells are part of the next lab.","title":"4. Skip Unnecessary Cells"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/","text":"Prepare for HyperParameter Tuning - Ideal Model Before you can perform hyperparameter tuning you must select the model you wish to tune. To do this you will run several models and evaluate quality metrics on the models. Once the ideal model has been selected you are ready to proceed to hyperparameter tuning. Identify Model for Tuning The ideal model for tuning is typically the model with the highest training accuracy per the quality metrics. As a first step you need to run the Jupyter Notebook with Kale enabled to generate model test results. Before running your model make sure that you are outputting model measurements. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. 1. Confirm Output from Eval Steps Notice that the eval_* steps contain code to print out model quality measurements. Each cell that performs evaluation should have a similar line of code. 2. Compile and Run Notebook with Kale Select Compile and Run to test the models using Kubeflow pipelines. Confirm successful execution by viewing the output from Kale. 3. Access Kubeflow Pipeline Click View next to Running pipeline to access the relevant Kubeflow pipelines logs for analysis. Once in the pipeline display scroll down to observe three pipelines, executed in parallel, for the three models. 4. Review A Model Click eval_lgbm and then click Logs to see the quality metric output. Scroll until you see the three numbers together, these are the output from the eval_step for each model. Recall that the metrics are presented as follows r squared mean squared error mean squared logarithmic error Take note of these three numbers. You will need them to compare against the other models in the subsequent lab.","title":"Preparing Hyperparameter Tuning Ideal Model"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#prepare-for-hyperparameter-tuning-ideal-model","text":"Before you can perform hyperparameter tuning you must select the model you wish to tune. To do this you will run several models and evaluate quality metrics on the models. Once the ideal model has been selected you are ready to proceed to hyperparameter tuning.","title":"Prepare for HyperParameter Tuning - Ideal Model"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#identify-model-for-tuning","text":"The ideal model for tuning is typically the model with the highest training accuracy per the quality metrics. As a first step you need to run the Jupyter Notebook with Kale enabled to generate model test results. Before running your model make sure that you are outputting model measurements. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Identify Model for Tuning"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#1-confirm-output-from-eval-steps","text":"Notice that the eval_* steps contain code to print out model quality measurements. Each cell that performs evaluation should have a similar line of code.","title":"1. Confirm Output from Eval Steps"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#2-compile-and-run-notebook-with-kale","text":"Select Compile and Run to test the models using Kubeflow pipelines. Confirm successful execution by viewing the output from Kale.","title":"2. Compile and Run Notebook with Kale"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#3-access-kubeflow-pipeline","text":"Click View next to Running pipeline to access the relevant Kubeflow pipelines logs for analysis. Once in the pipeline display scroll down to observe three pipelines, executed in parallel, for the three models.","title":"3. Access Kubeflow Pipeline"},{"location":"modules/notebook-katib-tuning/preparing-hyperparameter-tuning/#4-review-a-model","text":"Click eval_lgbm and then click Logs to see the quality metric output. Scroll until you see the three numbers together, these are the output from the eval_step for each model. Recall that the metrics are presented as follows r squared mean squared error mean squared logarithmic error Take note of these three numbers. You will need them to compare against the other models in the subsequent lab.","title":"4. Review A Model"},{"location":"modules/notebook-katib-tuning/upload-handouts/","text":"Get Dataset and Code To work through this module you will need the code and data we have provided. If you are not familiar with Kale we strongly recommend you complete Kale 101. If you do not have the completed notebook available you may download and unzip the handout . Upload the handout files Once you\u2019ve unzipped the handout, you should see the following files. 1. Review the handout files car_prices.csv is our data file. data_dictionary-carprices.xlsx contains examples images for the notebook and will need to be treated as a separate folder. predict_car_price_katib.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will build on the modified and annoted code from Kale 101 to define and run Katib Experiments in Kubeflow Pipelines! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step. 2. Open the learn-katib-pipelines-vol-1 folder Double-click on the directory, learn-katib-pipelines-vol-1 . 3. Click the file upload button 4. Upload handout files In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-katib-pipelines-vol-1 directory. 5. Create a new folder Click the button to create a new folder. 6. Name the folder \"data\" 7. Move data files Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder. 8. Open our notebook Double-click predict_car_price.ipynb in the file browser pane. 9. Enable Kale Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"Get Dataset and Code"},{"location":"modules/notebook-katib-tuning/upload-handouts/#get-dataset-and-code","text":"To work through this module you will need the code and data we have provided. If you are not familiar with Kale we strongly recommend you complete Kale 101. If you do not have the completed notebook available you may download and unzip the handout .","title":"Get Dataset and Code"},{"location":"modules/notebook-katib-tuning/upload-handouts/#upload-the-handout-files","text":"Once you\u2019ve unzipped the handout, you should see the following files.","title":"Upload the handout files"},{"location":"modules/notebook-katib-tuning/upload-handouts/#1-review-the-handout-files","text":"car_prices.csv is our data file. data_dictionary-carprices.xlsx contains examples images for the notebook and will need to be treated as a separate folder. predict_car_price_katib.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will build on the modified and annoted code from Kale 101 to define and run Katib Experiments in Kubeflow Pipelines! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step.","title":"1. Review the handout files"},{"location":"modules/notebook-katib-tuning/upload-handouts/#2-open-the-learn-katib-pipelines-vol-1-folder","text":"Double-click on the directory, learn-katib-pipelines-vol-1 .","title":"2. Open the learn-katib-pipelines-vol-1 folder"},{"location":"modules/notebook-katib-tuning/upload-handouts/#3-click-the-file-upload-button","text":"","title":"3. Click the file upload button"},{"location":"modules/notebook-katib-tuning/upload-handouts/#4-upload-handout-files","text":"In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-katib-pipelines-vol-1 directory.","title":"4. Upload handout files"},{"location":"modules/notebook-katib-tuning/upload-handouts/#5-create-a-new-folder","text":"Click the button to create a new folder.","title":"5. Create a new folder"},{"location":"modules/notebook-katib-tuning/upload-handouts/#6-name-the-folder-data","text":"","title":"6. Name the folder \"data\""},{"location":"modules/notebook-katib-tuning/upload-handouts/#7-move-data-files","text":"Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder.","title":"7. Move data files"},{"location":"modules/notebook-katib-tuning/upload-handouts/#8-open-our-notebook","text":"Double-click predict_car_price.ipynb in the file browser pane.","title":"8. Open our notebook"},{"location":"modules/notebook-katib-tuning/upload-handouts/#9-enable-kale","text":"Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"9. Enable Kale"},{"location":"modules/notebook-katib-tuning/labs/lab-identify-model/","text":"Lab: Identify \"ideal\" Model for Hyperparameter Tuning The notebook you executed evaluates three models in parallel. You must now select the ideal model for hyperparameter tuning. Requirements To identify the ideal model, evaluate all available outputs for model accuracy. In the prior section you looked at the quality metrics for one model. Now you must review the quality metrics for the remaining models. Once you have reviewed all three models you will be able to pick the best performing model. This is the model you will use for hyperparameter tuning. As a reminder for each of the metrics displayed r squared: generally a higher value indicates a better model fit mean squared error: generally a lower value indicates a better model fit mean squared logarithmic error: generally a lower value indicates a better model fit The ideal model is the model with the best quality metrics per the descriptions above. Solution View Solution Keeping in mind that these are algorithmic models numerical output may not always be the same. Generally after reviewing all three models via the same method presented in the prior activity we can identify that the xgb type has the best scores in each of the metric areas, therefore we will proceed with this model. In the next section you will isolate this model in the notebook and skip the other models so that your hyperparameter tuning focuses only on one model. At this point your Notebook should not be changed you should only have the ideal model identified. Please proceed to the next section.","title":"Lab: Identify Ideal Model"},{"location":"modules/notebook-katib-tuning/labs/lab-identify-model/#lab-identify-ideal-model-for-hyperparameter-tuning","text":"The notebook you executed evaluates three models in parallel. You must now select the ideal model for hyperparameter tuning.","title":"Lab: Identify \"ideal\" Model for Hyperparameter Tuning"},{"location":"modules/notebook-katib-tuning/labs/lab-identify-model/#requirements","text":"To identify the ideal model, evaluate all available outputs for model accuracy. In the prior section you looked at the quality metrics for one model. Now you must review the quality metrics for the remaining models. Once you have reviewed all three models you will be able to pick the best performing model. This is the model you will use for hyperparameter tuning. As a reminder for each of the metrics displayed r squared: generally a higher value indicates a better model fit mean squared error: generally a lower value indicates a better model fit mean squared logarithmic error: generally a lower value indicates a better model fit The ideal model is the model with the best quality metrics per the descriptions above.","title":"Requirements"},{"location":"modules/notebook-katib-tuning/labs/lab-identify-model/#solution","text":"View Solution Keeping in mind that these are algorithmic models numerical output may not always be the same. Generally after reviewing all three models via the same method presented in the prior activity we can identify that the xgb type has the best scores in each of the metric areas, therefore we will proceed with this model. In the next section you will isolate this model in the notebook and skip the other models so that your hyperparameter tuning focuses only on one model. At this point your Notebook should not be changed you should only have the ideal model identified. Please proceed to the next section.","title":"Solution"},{"location":"modules/notebook-katib-tuning/labs/lab-interpret-katib-output/","text":"Lab: Interpret Katib Output There are two ways to identify the ideal hyperparameters for model optimization. Requirement Identify the ideal hyperparameters by reviewing the output from either the Experiments (KFP) UI or the Experiments (AutoML) UI. Hint View Hint In the Experiments (KFP) UI you need to select the experiment set and review the column for the desired objective. In the Experiments (AutoML) UI you can either Scroll down to the summary section. Navigate to the Trials tab and view the highlighted trial. Solutions View Solution For Experiments(KFP) Select the experiment with the best objective metric score, click the Config Tab, scroll down to the Parameters. For Experiments(KFP) Scroll down to the Parameters: To see which trial produced the best result select Trials and identify the highlighted row:","title":"Lab: Interpret Katib Output"},{"location":"modules/notebook-katib-tuning/labs/lab-interpret-katib-output/#lab-interpret-katib-output","text":"There are two ways to identify the ideal hyperparameters for model optimization.","title":"Lab: Interpret Katib Output"},{"location":"modules/notebook-katib-tuning/labs/lab-interpret-katib-output/#requirement","text":"Identify the ideal hyperparameters by reviewing the output from either the Experiments (KFP) UI or the Experiments (AutoML) UI.","title":"Requirement"},{"location":"modules/notebook-katib-tuning/labs/lab-interpret-katib-output/#hint","text":"View Hint In the Experiments (KFP) UI you need to select the experiment set and review the column for the desired objective. In the Experiments (AutoML) UI you can either Scroll down to the summary section. Navigate to the Trials tab and view the highlighted trial.","title":"Hint"},{"location":"modules/notebook-katib-tuning/labs/lab-interpret-katib-output/#solutions","text":"View Solution For Experiments(KFP) Select the experiment with the best objective metric score, click the Config Tab, scroll down to the Parameters. For Experiments(KFP) Scroll down to the Parameters: To see which trial produced the best result select Trials and identify the highlighted row:","title":"Solutions"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/","text":"Lab: Set Up Katib Job Setting up the Katib Job takes place within the Kale UI either within MiniKF or Enterprise KubeFlow deployment with Arrikto. By setting up the job from within the Kale UI you are ensuring that Katib can take advantage of the KubeFlow pipelines and the Rok snapshotting. The preparation done in advance ensures that when you go to configure a Katib Job all the options will be prepopulated or will be based on what has been defined in the notebook cells. Requirements To configure the hyperparameter tuning job you will need to configure each of the below: Search Space Parameters All hyperparameters are to be considered with the following details: NUM_ESTIMATORS should start at 400 and go to 600 at intervals of 10. MAX_DEPTH should be between 1 and 5 at intervals of 1. LEARNING_RATE should be between 0 and 1 at intervals of 0.1. Search Algorithm This should be Grid Search. Search Objective You are trying to maximize the value of r squared. Run Parameters You should do at last 3 trials, at most 12 and stop after 3 failed attempts. Please configure the Katib Job to meet the above requirements. Solution View Solution The solution is displayed in the screenshots below: Next Steps Once you have confirmed that your solution is consistent with what is expected click Compile and Run to begin execution.","title":"Lab: Set Up Katib Job"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#lab-set-up-katib-job","text":"Setting up the Katib Job takes place within the Kale UI either within MiniKF or Enterprise KubeFlow deployment with Arrikto. By setting up the job from within the Kale UI you are ensuring that Katib can take advantage of the KubeFlow pipelines and the Rok snapshotting. The preparation done in advance ensures that when you go to configure a Katib Job all the options will be prepopulated or will be based on what has been defined in the notebook cells.","title":"Lab: Set Up Katib Job"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#requirements","text":"To configure the hyperparameter tuning job you will need to configure each of the below:","title":"Requirements"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#search-space-parameters","text":"All hyperparameters are to be considered with the following details: NUM_ESTIMATORS should start at 400 and go to 600 at intervals of 10. MAX_DEPTH should be between 1 and 5 at intervals of 1. LEARNING_RATE should be between 0 and 1 at intervals of 0.1.","title":"Search Space Parameters"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#search-algorithm","text":"This should be Grid Search.","title":"Search Algorithm"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#search-objective","text":"You are trying to maximize the value of r squared.","title":"Search Objective"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#run-parameters","text":"You should do at last 3 trials, at most 12 and stop after 3 failed attempts. Please configure the Katib Job to meet the above requirements.","title":"Run Parameters"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#solution","text":"View Solution The solution is displayed in the screenshots below:","title":"Solution"},{"location":"modules/notebook-katib-tuning/labs/lab-set-up-katib-job/#next-steps","text":"Once you have confirmed that your solution is consistent with what is expected click Compile and Run to begin execution.","title":"Next Steps"},{"location":"modules/notebook-katib-tuning/labs/lab-tuning-preparation/","text":"Lab: Hyperparameter Tuning Preparation Preparing your notebook for hyperparameter tuning involves: Moving all relevant tracking metrics to the final cell of a notebook and tagging this cell with Pipeline Metrics . Skipping all unnecessary cells so you only focus on the relevant model. In the prior walk through you began this exercise and now in this lab you will finish the work you began. Requirement This lab has two requirements. First you must finish preparing the Pipeline Metrics cell. To do this you must: Add print(xgb_sq_err) to the final cell Add print(xgb_sq_log_err) to the final cell Tag the cell with Pipeline Metrics Second you must finish skipping unnecessary cells. To do this tag the Random Forest model cells with Skip . Solution View Solution The final cell in the notebook should look like this: The skipped cells should look like this: Once you have completed this lab please proceed to the next page.","title":"Lab: Hyperparameter Tuning Preparation"},{"location":"modules/notebook-katib-tuning/labs/lab-tuning-preparation/#lab-hyperparameter-tuning-preparation","text":"Preparing your notebook for hyperparameter tuning involves: Moving all relevant tracking metrics to the final cell of a notebook and tagging this cell with Pipeline Metrics . Skipping all unnecessary cells so you only focus on the relevant model. In the prior walk through you began this exercise and now in this lab you will finish the work you began.","title":"Lab: Hyperparameter Tuning Preparation"},{"location":"modules/notebook-katib-tuning/labs/lab-tuning-preparation/#requirement","text":"This lab has two requirements. First you must finish preparing the Pipeline Metrics cell. To do this you must: Add print(xgb_sq_err) to the final cell Add print(xgb_sq_log_err) to the final cell Tag the cell with Pipeline Metrics Second you must finish skipping unnecessary cells. To do this tag the Random Forest model cells with Skip .","title":"Requirement"},{"location":"modules/notebook-katib-tuning/labs/lab-tuning-preparation/#solution","text":"View Solution The final cell in the notebook should look like this: The skipped cells should look like this: Once you have completed this lab please proceed to the next page.","title":"Solution"},{"location":"modules/notebook-servers/notebook-server-base-image/","text":"Launch a Notebook Server from a Base Image To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below. 1. View the Home screen in your MiniKF Kubeflow deployment. 2. Select the Notebooks pane from the main navigation menu. You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty. 3. Click the NEW SERVER button. Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server. 4. Enter a name. In the Name field, enter a name, e.g., learn-kubeflow-pipelines. 5. Add a data volume. Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you. 6. Click the LAUNCH button. Scroll to the bottom of the form and click the LAUNCH button to create your notebook server. 7. Connect to your notebook server. To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"Deploy a Notebook Server"},{"location":"modules/notebook-servers/notebook-server-base-image/#launch-a-notebook-server-from-a-base-image","text":"To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below.","title":"Launch a Notebook Server from a Base Image"},{"location":"modules/notebook-servers/notebook-server-base-image/#1-view-the-home-screen-in-your-minikf-kubeflow-deployment","text":"","title":"1. View the Home screen in your MiniKF Kubeflow deployment."},{"location":"modules/notebook-servers/notebook-server-base-image/#2-select-the-notebooks-pane-from-the-main-navigation-menu","text":"You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty.","title":"2. Select the Notebooks pane from the main navigation menu."},{"location":"modules/notebook-servers/notebook-server-base-image/#3-click-the-new-server-button","text":"Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server.","title":"3. Click the NEW SERVER button."},{"location":"modules/notebook-servers/notebook-server-base-image/#4-enter-a-name","text":"In the Name field, enter a name, e.g., learn-kubeflow-pipelines.","title":"4. Enter a name."},{"location":"modules/notebook-servers/notebook-server-base-image/#5-add-a-data-volume","text":"Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you.","title":"5. Add a data volume."},{"location":"modules/notebook-servers/notebook-server-base-image/#6-click-the-launch-button","text":"Scroll to the bottom of the form and click the LAUNCH button to create your notebook server.","title":"6. Click the LAUNCH button."},{"location":"modules/notebook-servers/notebook-server-base-image/#7-connect-to-your-notebook-server","text":"To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"7. Connect to your notebook server."},{"location":"modules/notebook-to-pipeline/","text":"Kale 101 : Transform Jupyter Notebooks into Kubeflow Pipelines Course Summary When first learning to use Kubeflow, you will probably want to adapt one or more existing Python scripts or Jupyter notebooks so that they can be deployed as a Kubeflow pipeline. In this module we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We\u2019ll teach you how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension.","title":"Home"},{"location":"modules/notebook-to-pipeline/#kale-101-transform-jupyter-notebooks-into-kubeflow-pipelines","text":"","title":"Kale 101: Transform Jupyter Notebooks into Kubeflow Pipelines"},{"location":"modules/notebook-to-pipeline/#course-summary","text":"When first learning to use Kubeflow, you will probably want to adapt one or more existing Python scripts or Jupyter notebooks so that they can be deployed as a Kubeflow pipeline. In this module we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We\u2019ll teach you how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension.","title":"Course Summary"},{"location":"modules/notebook-to-pipeline/checkpoint-fundamentals/","text":"Checkpoint: Kale Fundamentals At this point, we\u2019ve covered the Pipeline Step , Imports , and Skip Cell annotations. With a little review here and there you should now be able to recall how to do the following: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step. Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells.","title":"Checkpoint: Kale Fundamentals"},{"location":"modules/notebook-to-pipeline/checkpoint-fundamentals/#checkpoint-kale-fundamentals","text":"At this point, we\u2019ve covered the Pipeline Step , Imports , and Skip Cell annotations. With a little review here and there you should now be able to recall how to do the following: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step. Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells.","title":"Checkpoint: Kale Fundamentals"},{"location":"modules/notebook-to-pipeline/introduction/","text":"Course Overview","title":"Course Overview"},{"location":"modules/notebook-to-pipeline/introduction/#course-overview","text":"","title":"Course Overview"},{"location":"modules/notebook-to-pipeline/minikf-gcp/","text":"Set Up MiniKF on Google Cloud Two Options You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions. Follow the Set Up Video Follow the Step-by-Step Instructions 1. Find MiniKF in the Google Cloud Marketplace. Open Google Cloud Marketplace and search for \"MiniKF\". 2. Select the MiniKF virtual machine by Arrikto. 3. Click the LAUNCH button and select your project. 4. Choose a name and zone for your MiniKF instance. In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button. 5. Wait for the MiniKF compute instance to boot up. 6. SSH to MiniKF When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete. 7. Log in to MiniKF When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"Deploy MiniKF on GCP"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#set-up-minikf-on-google-cloud","text":"","title":"Set Up MiniKF on Google Cloud"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#two-options","text":"You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions.","title":"Two Options"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#follow-the-set-up-video","text":"","title":"Follow the Set Up Video"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#follow-the-step-by-step-instructions","text":"","title":"Follow the Step-by-Step Instructions"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#1-find-minikf-in-the-google-cloud-marketplace","text":"Open Google Cloud Marketplace and search for \"MiniKF\".","title":"1. Find MiniKF in the Google Cloud Marketplace."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#2-select-the-minikf-virtual-machine-by-arrikto","text":"","title":"2. Select the MiniKF virtual machine by Arrikto."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#3-click-the-launch-button-and-select-your-project","text":"","title":"3. Click the LAUNCH button and select your project."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#4-choose-a-name-and-zone-for-your-minikf-instance","text":"In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button.","title":"4. Choose a name and zone for your MiniKF instance."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#5-wait-for-the-minikf-compute-instance-to-boot-up","text":"","title":"5. Wait for the MiniKF compute instance to boot up."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#6-ssh-to-minikf","text":"When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete.","title":"6. SSH to MiniKF"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#7-log-in-to-minikf","text":"When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"7. Log in to MiniKF"},{"location":"modules/notebook-to-pipeline/upload-handouts/","text":"Get Dataset and Code To work through this module you will need the code and data we have provided. Please download and unzip the handout . Upload the handout files Once you\u2019ve unzipped the handout, you should see the following files. 1. Review the handout files car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step. 2. Open the learn-kubeflow-pipelines-vol-1 folder Double-click on the directory, learn-kubeflow-pipelines-vol-1 . 3. Click the file upload button 4. Upload handout files In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-kubeflow-pipelines-vol-1 directory. 5. Create a new folder Click the button to create a new folder. 6. Name the folder \"data\" 7. Move data files Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder. 8. Open our notebook Double-click predict_car_price.ipynb in the file browser pane. 9. Enable Kale Click the Enable toggle in the Kale Deployment panel to enable Kale. 10. Launch a Terminal Click the Launcher tab and launch a terminal. 11. Install Requirements In the terminal enter the following commands. Change to the learn-kubeflow-pipelines-vol-1 directory. cd learn-kubeflow-pipelines-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt 12. Restart the Kernel In the predict_car_price.ipynb notebook, restart the kernel.","title":"Get Dataset and Code"},{"location":"modules/notebook-to-pipeline/upload-handouts/#get-dataset-and-code","text":"To work through this module you will need the code and data we have provided. Please download and unzip the handout .","title":"Get Dataset and Code"},{"location":"modules/notebook-to-pipeline/upload-handouts/#upload-the-handout-files","text":"Once you\u2019ve unzipped the handout, you should see the following files.","title":"Upload the handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#1-review-the-handout-files","text":"car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step.","title":"1. Review the handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#2-open-the-learn-kubeflow-pipelines-vol-1-folder","text":"Double-click on the directory, learn-kubeflow-pipelines-vol-1 .","title":"2. Open the learn-kubeflow-pipelines-vol-1 folder"},{"location":"modules/notebook-to-pipeline/upload-handouts/#3-click-the-file-upload-button","text":"","title":"3. Click the file upload button"},{"location":"modules/notebook-to-pipeline/upload-handouts/#4-upload-handout-files","text":"In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-kubeflow-pipelines-vol-1 directory.","title":"4. Upload handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#5-create-a-new-folder","text":"Click the button to create a new folder.","title":"5. Create a new folder"},{"location":"modules/notebook-to-pipeline/upload-handouts/#6-name-the-folder-data","text":"","title":"6. Name the folder \"data\""},{"location":"modules/notebook-to-pipeline/upload-handouts/#7-move-data-files","text":"Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder.","title":"7. Move data files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#8-open-our-notebook","text":"Double-click predict_car_price.ipynb in the file browser pane.","title":"8. Open our notebook"},{"location":"modules/notebook-to-pipeline/upload-handouts/#9-enable-kale","text":"Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"9. Enable Kale"},{"location":"modules/notebook-to-pipeline/upload-handouts/#10-launch-a-terminal","text":"Click the Launcher tab and launch a terminal.","title":"10. Launch a Terminal"},{"location":"modules/notebook-to-pipeline/upload-handouts/#11-install-requirements","text":"In the terminal enter the following commands. Change to the learn-kubeflow-pipelines-vol-1 directory. cd learn-kubeflow-pipelines-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt","title":"11. Install Requirements"},{"location":"modules/notebook-to-pipeline/upload-handouts/#12-restart-the-kernel","text":"In the predict_car_price.ipynb notebook, restart the kernel.","title":"12. Restart the Kernel"},{"location":"modules/notebook-to-pipeline/working-example/","text":"Notebook Walkthrough","title":"Notebook Walkthrough"},{"location":"modules/notebook-to-pipeline/working-example/#notebook-walkthrough","text":"","title":"Notebook Walkthrough"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/","text":"Solution - Lab: Skip Cells Requirements Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise. Solution We definitely need the first cell in the Clean Data section so no changes are required here. This step, however, displays the data types for fields in this data set. We use the output to identify the fact that we need to change the data type for the \u2018symboling\u2019 column to \u2018str\u2019. However, we do not need to execute the cell on each run of the pipeline we\u2019re building. The first two cells in the block below work together to fix some misspellings and ensure that we are using the same label for \u2018CarName\u2019 across all rows. The cell below is diagnostic. It turns out that we don\u2019t have any duplicates in this data set, but even if we did, we do not need to run this cell as part of the clean_data step or any pipeline step. We would only run cells that dealt with duplicates. This cell is just a means of getting a quick peek at the data. It doesn\u2019t do any data cleaning. !!! important \"Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution above.","title":"Solution - Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#solution-lab-skip-cells","text":"","title":"Solution - Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#requirements","text":"Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#solution","text":"We definitely need the first cell in the Clean Data section so no changes are required here. This step, however, displays the data types for fields in this data set. We use the output to identify the fact that we need to change the data type for the \u2018symboling\u2019 column to \u2018str\u2019. However, we do not need to execute the cell on each run of the pipeline we\u2019re building. The first two cells in the block below work together to fix some misspellings and ensure that we are using the same label for \u2018CarName\u2019 across all rows. The cell below is diagnostic. It turns out that we don\u2019t have any duplicates in this data set, but even if we did, we do not need to run this cell as part of the clean_data step or any pipeline step. We would only run cells that dealt with duplicates. This cell is just a means of getting a quick peek at the data. It doesn\u2019t do any data cleaning. !!! important \"Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution above.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/","text":"Lab: Skip Cells Now, as a lab exercise, take a look at all the cells currently included in the clean_data step and update those cells to meet the requirements specified below. Requirements Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise. Solution Once you\u2019ve made the updates you believe are required, take a look at the solution and verify that you\u2019ve made the right changes. Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution we've provided.","title":"Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#lab-skip-cells","text":"Now, as a lab exercise, take a look at all the cells currently included in the clean_data step and update those cells to meet the requirements specified below.","title":"Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#requirements","text":"Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#solution","text":"Once you\u2019ve made the updates you believe are required, take a look at the solution and verify that you\u2019ve made the right changes. Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution we've provided.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/","text":"Lab: Skip Section To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined. Requirements Update your copy of our notebook so that all cells in the Visualize Data section of our notebook will be excluded from the Kubeflow pipeline we are building. Solution Expand to see solution When you\u2019re done, every cell in the Visualize Data section should be marked with the Skip Cell annotation. In total there should be six cells you\u2019ll need to annotate as a Skip Cell.","title":"Lab: Skip Section"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#lab-skip-section","text":"To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined.","title":"Lab: Skip Section"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#requirements","text":"Update your copy of our notebook so that all cells in the Visualize Data section of our notebook will be excluded from the Kubeflow pipeline we are building.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#solution","text":"Expand to see solution When you\u2019re done, every cell in the Visualize Data section should be marked with the Skip Cell annotation. In total there should be six cells you\u2019ll need to annotate as a Skip Cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/skip-cells/","text":"Skip Cells Not Used in a Step In their current form, our read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in our notebook where it will be of use as we continue to develop our models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. Follow Along Please follow along in your own copy of our notebook as we annotate skip cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below.","title":"Skip Cells Not Used in a Step"},{"location":"modules/notebook-to-pipeline/exclude/skip-cells/#skip-cells-not-used-in-a-step","text":"In their current form, our read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in our notebook where it will be of use as we continue to develop our models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. Follow Along Please follow along in your own copy of our notebook as we annotate skip cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below.","title":"Skip Cells Not Used in a Step"},{"location":"modules/notebook-to-pipeline/exclude/skip-sections/","text":"Skip Sections Not Used in Pipelines The Visualize Data section of this notebook informs how we build models. It is what we used to identify the significant independent variables in this dataset as reflected in this portion of the notebook. We use the outcome of our analysis to define the list sig_col near the top of the Modeling section. We don\u2019t need to run the cells in the Visualize Data section as part of our pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine our models further and want to return to a visualization and analysis phase in a future iteration.","title":"Skip Sections Not Used"},{"location":"modules/notebook-to-pipeline/exclude/skip-sections/#skip-sections-not-used-in-pipelines","text":"The Visualize Data section of this notebook informs how we build models. It is what we used to identify the significant independent variables in this dataset as reflected in this portion of the notebook. We use the outcome of our analysis to define the list sig_col near the top of the Modeling section. We don\u2019t need to run the cells in the Visualize Data section as part of our pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine our models further and want to return to a visualization and analysis phase in a future iteration.","title":"Skip Sections Not Used in Pipelines"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/","text":"Solution - Lab: Create prep_data Step Requirements To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include only cells that contain code that is core to this step. Exclude cells in the Prep Data section that are not core to the functionality of this step using the Skip Cell annotation. Solution Requirements 1, 2, 3, and 4: Apply the Pipeline Step annotation to the first cell in the Prep Data section to create a new pipeline step. Specify prep_data as the value for the Step name parameter. Specify clean_data as the step on which prep_data depends. We use clean_data here rather than read_data because we want the data in the df_auto data frame after it has been cleaned up by the operations in the clean_data step. These cells select just the significant columns we identified during data analysis. It is only these that we want to use in training our model, so these two cells are essential to the prep_data step. Our pipeline can now be depicted as: Requirement 5: This cell simply produces some diagnostic output used when we were developing this notebook to ensure that the data frame was being modified to include only the significant columns. We annotate it as a skip cell. Requirement 4: This cell transforms the categorical variables in our data set so that they can be used in training our model. It is essential to the prep_data step. Requirement 5: This cell produces diagnostic output. We annotate it as a skip cell.","title":"Solution - Lab: Create prep_data"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#solution-lab-create-prep_data-step","text":"","title":"Solution - Lab: Create prep_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#requirements","text":"To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include only cells that contain code that is core to this step. Exclude cells in the Prep Data section that are not core to the functionality of this step using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#solution","text":"Requirements 1, 2, 3, and 4: Apply the Pipeline Step annotation to the first cell in the Prep Data section to create a new pipeline step. Specify prep_data as the value for the Step name parameter. Specify clean_data as the step on which prep_data depends. We use clean_data here rather than read_data because we want the data in the df_auto data frame after it has been cleaned up by the operations in the clean_data step. These cells select just the significant columns we identified during data analysis. It is only these that we want to use in training our model, so these two cells are essential to the prep_data step. Our pipeline can now be depicted as: Requirement 5: This cell simply produces some diagnostic output used when we were developing this notebook to ensure that the data frame was being modified to include only the significant columns. We annotate it as a skip cell. Requirement 4: This cell transforms the categorical variables in our data set so that they can be used in training our model. It is essential to the prep_data step. Requirement 5: This cell produces diagnostic output. We annotate it as a skip cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/","text":"Lab: Create prep_data step In the Prep Data section of our notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for our regression models and only including these columns in our data frame. Encoding categorical variables so that they can be used by our models. Requirements To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include all cells that contain code that is core to the step prep_data . Exclude cells in the Prep Data section that are not core to the functionality of this step. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Create prep_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#lab-create-prep_data-step","text":"In the Prep Data section of our notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for our regression models and only including these columns in our data frame. Encoding categorical variables so that they can be used by our models.","title":"Lab: Create prep_data step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#requirements","text":"To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include all cells that contain code that is core to the step prep_data . Exclude cells in the Prep Data section that are not core to the functionality of this step.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/","text":"Solution - Lab: Create split_data Step In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow. Requirements Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data . Solution Requirement 1: We accomplish splitting the data in just one cell. Annotate this cell as depicted below to create the split_data pipeline step. Requirement 2: The split_data step depends on the output of prep_data which selects just the significant columns in our dataset and transforms categorical columns for training. Our pipeline can now be depicted as:","title":"Solution - Lab: Create split_data"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#solution-lab-create-split_data-step","text":"In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow.","title":"Solution - Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#requirements","text":"Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data .","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#solution","text":"Requirement 1: We accomplish splitting the data in just one cell. Annotate this cell as depicted below to create the split_data pipeline step. Requirement 2: The split_data step depends on the output of prep_data which selects just the significant columns in our dataset and transforms categorical columns for training. Our pipeline can now be depicted as:","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/","text":"Lab: Create split_data Step In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow. Requirements Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data . Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#lab-create-split_data-step","text":"In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow.","title":"Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#requirements","text":"Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data .","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/example/","text":"Apply the Imports Annotation We\u2019ll do a bit more reorganization in the Modeling section later in this module, so for now, let\u2019s just focus on the portion of our notebook up to the Modeling section. Follow Along Please follow along in your own copy of our notebook as we address these imports. Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. They are as follows. Let's move these imports so that they are together with the others in the first cell of our notebook. Noce we've done that, the first cell in our notebook will look something like this. The modified cells in the Visualize Data section should look something like this. Now, let\u2019s apply the Imports annotation to the cell containing our reorganized imports.","title":"Apply the Imports Annotation"},{"location":"modules/notebook-to-pipeline/imports/example/#apply-the-imports-annotation","text":"We\u2019ll do a bit more reorganization in the Modeling section later in this module, so for now, let\u2019s just focus on the portion of our notebook up to the Modeling section. Follow Along Please follow along in your own copy of our notebook as we address these imports. Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. They are as follows. Let's move these imports so that they are together with the others in the first cell of our notebook. Noce we've done that, the first cell in our notebook will look something like this. The modified cells in the Visualize Data section should look something like this. Now, let\u2019s apply the Imports annotation to the cell containing our reorganized imports.","title":"Apply the Imports Annotation"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/","text":"Solution - Lab: Imports Requirements Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell. Solution In the Modeling section of our notebook, there are import statements in the Prep Data and Build Models subsections. In the Prep Data subsection, we have this cell. In the Build Models subsection, we have these cells. To satisfy requirements 1 and 2 , relocate the import statements from these three cells to a new cell just below the \u201cModeling\u201d header. When you\u2019re done: The new cell should look like the cell depicted below. The three cells depicted above should not contain any import statements. The third cell above should be removed since it will be empty after relocating the import statements it contains. To satisfy requirement 3 , apply the Imports annotation to our new cell.","title":"Solution - Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#solution-lab-imports","text":"","title":"Solution - Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#requirements","text":"Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#solution","text":"In the Modeling section of our notebook, there are import statements in the Prep Data and Build Models subsections. In the Prep Data subsection, we have this cell. In the Build Models subsection, we have these cells. To satisfy requirements 1 and 2 , relocate the import statements from these three cells to a new cell just below the \u201cModeling\u201d header. When you\u2019re done: The new cell should look like the cell depicted below. The three cells depicted above should not contain any import statements. The third cell above should be removed since it will be empty after relocating the import statements it contains. To satisfy requirement 3 , apply the Imports annotation to our new cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/","text":"Lab: Imports Organize and annotate a second Imports cell in your copy of our notebook. This cell should include all import statements currently included in cells found in the Modeling section of our notebook. Requirements Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#lab-imports","text":"Organize and annotate a second Imports cell in your copy of our notebook. This cell should include all import statements currently included in cells found in the Modeling section of our notebook.","title":"Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#requirements","text":"Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/","text":"Gather Imports in One Cell Why? In general, though convenient, it\u2019s usually not a good idea to scatter imports throughout your code. It almost always leads to errors caused by references to modules that you end up using before the location where you had imported them originally. When building pipelines with Kale, it\u2019s essential that you organize your imports together in one or two cells and then annotate those cells using the Imports label. This will avoid pipeline execution errors due to missing modules and enable Kale to configure the execution environment for each pipeline step correctly. How Kale uses Imports cells Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell will execute before the core code for the pipeline step. Organizing imports together and annotating those cells as Imports ensures that Kale can organize the code for each step and set up its execution environment correctly.","title":"Gather Imports in One Cell"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#gather-imports-in-one-cell","text":"","title":"Gather Imports in One Cell"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#why","text":"In general, though convenient, it\u2019s usually not a good idea to scatter imports throughout your code. It almost always leads to errors caused by references to modules that you end up using before the location where you had imported them originally. When building pipelines with Kale, it\u2019s essential that you organize your imports together in one or two cells and then annotate those cells using the Imports label. This will avoid pipeline execution errors due to missing modules and enable Kale to configure the execution environment for each pipeline step correctly.","title":"Why?"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#how-kale-uses-imports-cells","text":"Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell will execute before the core code for the pipeline step. Organizing imports together and annotating those cells as Imports ensures that Kale can organize the code for each step and set up its execution environment correctly.","title":"How Kale uses Imports cells"},{"location":"modules/notebook-to-pipeline/overview/03-objective/","text":"Our Objective for this Module Though we believe the data science solution in our working example is reasonable, for the rest of this module we will not focus on the implementation. Instead, we will focus on a realistic example of the considerations you will want to make and the process you will need to follow to transform an existing notebook written in Python into a Kubeflow pipeline that will run on a Kubernetes cluster.","title":"Our Objective for this Module"},{"location":"modules/notebook-to-pipeline/overview/03-objective/#our-objective-for-this-module","text":"Though we believe the data science solution in our working example is reasonable, for the rest of this module we will not focus on the implementation. Instead, we will focus on a realistic example of the considerations you will want to make and the process you will need to follow to transform an existing notebook written in Python into a Kubeflow pipeline that will run on a Kubernetes cluster.","title":"Our Objective for this Module"},{"location":"modules/notebook-to-pipeline/pipeline-design/checkpoint/","text":"Checkpoint: Pipeline Design Having completed the lessons in this module, you should now be able to do the following comfortably: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step . Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify single-step and multi-step dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Create pipeline branches that can run in parallel using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells. Organize and annotate Functions cells.","title":"Checkpoint: Pipeline Design"},{"location":"modules/notebook-to-pipeline/pipeline-design/checkpoint/#checkpoint-pipeline-design","text":"Having completed the lessons in this module, you should now be able to do the following comfortably: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step . Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify single-step and multi-step dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Create pipeline branches that can run in parallel using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells. Organize and annotate Functions cells.","title":"Checkpoint: Pipeline Design"},{"location":"modules/notebook-to-pipeline/pipeline-design/example/","text":"Apply this Design Principle As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Apply this Design Principle"},{"location":"modules/notebook-to-pipeline/pipeline-design/example/#apply-this-design-principle","text":"As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Apply this Design Principle"},{"location":"modules/notebook-to-pipeline/pipeline-design/functions.cells/","text":"Functions Cells One annotation we\u2019ve not yet discussed is the Functions cell type. Kale provides this cell type to enable you to identify blocks of code containing: Functions used later in your machine learning pipeline. Global variable definitions (other than pipeline parameters) and code that initializes lists, dictionaries, objects, and other values used throughout your pipeline. Functions cells help Kale identify all dependencies for pipeline steps. As you know, Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . In addition, Kale also prepends the code in every Functions cell as part of this process. Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by all Functions cells, and then followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell and all functions and other declarations found in all Functions cells will execute before the core code for the pipeline step. Organizing functions together and annotating those cells with the Functions label ensures that Kale can organize the code for each step and set up its execution environment correctly. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. Let\u2019s get some experience with Functions cells by abstracting out the scoring and output functionality from our evaluation steps. The last several lines for each of our evaluation steps are nearly identical. Let\u2019s write two functions to handle this logic and place them in a cell we\u2019ll annotate using the Functions label. We\u2019ll place this cell at the very top of the Build Models subsection Then let's modify each of our evaluation steps accordingly. This simple example illustrates the purpose of Functions cells and how to create and annotate them. In later modules, we\u2019ll make greater use of Functions cells.","title":"Functions Cells"},{"location":"modules/notebook-to-pipeline/pipeline-design/functions.cells/#functions-cells","text":"One annotation we\u2019ve not yet discussed is the Functions cell type. Kale provides this cell type to enable you to identify blocks of code containing: Functions used later in your machine learning pipeline. Global variable definitions (other than pipeline parameters) and code that initializes lists, dictionaries, objects, and other values used throughout your pipeline. Functions cells help Kale identify all dependencies for pipeline steps. As you know, Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . In addition, Kale also prepends the code in every Functions cell as part of this process. Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by all Functions cells, and then followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell and all functions and other declarations found in all Functions cells will execute before the core code for the pipeline step. Organizing functions together and annotating those cells with the Functions label ensures that Kale can organize the code for each step and set up its execution environment correctly. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. Let\u2019s get some experience with Functions cells by abstracting out the scoring and output functionality from our evaluation steps. The last several lines for each of our evaluation steps are nearly identical. Let\u2019s write two functions to handle this logic and place them in a cell we\u2019ll annotate using the Functions label. We\u2019ll place this cell at the very top of the Build Models subsection Then let's modify each of our evaluation steps accordingly. This simple example illustrates the purpose of Functions cells and how to create and annotate them. In later modules, we\u2019ll make greater use of Functions cells.","title":"Functions Cells"},{"location":"modules/notebook-to-pipeline/pipeline-design/iteration/","text":"Design for Iteration To complete our pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It doesn\u2019t make sense to combine the model training and evaluation code in a single cell or step as we have it in the example depicted below. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. Important Break your pipeline down to separate all discrete steps you might want to iterate on independently from other components of your workflow. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Design for Iteration"},{"location":"modules/notebook-to-pipeline/pipeline-design/iteration/#design-for-iteration","text":"To complete our pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It doesn\u2019t make sense to combine the model training and evaluation code in a single cell or step as we have it in the example depicted below. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. Important Break your pipeline down to separate all discrete steps you might want to iterate on independently from other components of your workflow. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Design for Iteration"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/","text":"Solution - Lab: RF train and eval steps Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution Requirements 1, 4 : Following the example for the LGBM model, we need to isolate the code that creates and trains the RandomForest model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm , this step has split_data as a dependency. This creates a branch in our pipeline. Requirement 5 : This cell just prints a diagnostic message. We can skip it in pipeline execution. Requirements 2, 4 : The code for the eval_rf step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_rf (uses the value of rf_model ) and indirectly on split_data because it uses the data values referenced by the variables: x , y , xt , and yt .","title":"Solution - Lab: RF train and eval"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#solution-lab-rf-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell.","title":"Solution - Lab: RF train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#requirements","text":"Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#solution","text":"Requirements 1, 4 : Following the example for the LGBM model, we need to isolate the code that creates and trains the RandomForest model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm , this step has split_data as a dependency. This creates a branch in our pipeline. Requirement 5 : This cell just prints a diagnostic message. We can skip it in pipeline execution. Requirements 2, 4 : The code for the eval_rf step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_rf (uses the value of rf_model ) and indirectly on split_data because it uses the data values referenced by the variables: x , y , xt , and yt .","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/","text":"Lab: Create a train_rf and eval_rf steps Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: RF train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#lab-create-a-train_rf-and-eval_rf-steps","text":"Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell.","title":"Lab: Create a train_rf and eval_rf steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#requirements","text":"Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/","text":"Solution - Lab: XGB train and eval steps Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution Requirements 1, 4 : Following the example for the XGB model, we need to isolate the code that creates and trains the XGB model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm and train_xgb , this step has split_data as a dependency. Requirement 5 : There are no cells to skip for this lab. Requirements 2, 4 : The code for the eval_xgb step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we do want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_xgb (uses the value of xgb_grid ) and indirectly on split_data because it uses the data values referenced by the variables: xt and yt .","title":"Solution - Lab: XGB train and eval"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#solution-lab-xgb-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell.","title":"Solution - Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#requirements","text":"Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#solution","text":"Requirements 1, 4 : Following the example for the XGB model, we need to isolate the code that creates and trains the XGB model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm and train_xgb , this step has split_data as a dependency. Requirement 5 : There are no cells to skip for this lab. Requirements 2, 4 : The code for the eval_xgb step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we do want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_xgb (uses the value of xgb_grid ) and indirectly on split_data because it uses the data values referenced by the variables: xt and yt .","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/","text":"Lab: XGB train and eval steps Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#lab-xgb-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell.","title":"Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#requirements","text":"Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/run-our-pipeline/","text":"Run Our Pipeline Now it\u2019s time to run our pipeline. To do this: Open the Kale Deployment panel, by clicking on the Kale icon in your Jupyter notebook environment. Click the pull-down menu for Select experiment and create a new experiment called car-price. Enter \u201cpredict-car-price\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for our models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of our models. Feel free to explore other output tabs and other aspects of the pipeline run. We\u2019ll address everything you see here in upcoming training modules.","title":"Run Our Pipeline"},{"location":"modules/notebook-to-pipeline/pipeline-design/run-our-pipeline/#run-our-pipeline","text":"Now it\u2019s time to run our pipeline. To do this: Open the Kale Deployment panel, by clicking on the Kale icon in your Jupyter notebook environment. Click the pull-down menu for Select experiment and create a new experiment called car-price. Enter \u201cpredict-car-price\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for our models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of our models. Feel free to explore other output tabs and other aspects of the pipeline run. We\u2019ll address everything you see here in upcoming training modules.","title":"Run Our Pipeline"},{"location":"modules/notebook-to-pipeline/pipeline-design/steps-with-multiple-dependencies/","text":"Steps w/ Multiple Dependencies In many cases, you\u2019ll need to create a step that depends on two or more other steps. Steps for producing output are one example. Let\u2019s take a look at how to set multiple dependencies by creating a final output step in our notebook. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. With the changes we\u2019ve just made to create a Functions cell containing functions for scoring and output to our notebook, each of our evaluation steps now calls output_results as its last statement. We can see this, for example in eval_xgb . Let\u2019s move the calls to output_results from each of our evaluation steps to a new step called output. Let\u2019s place the cell as the very last cell in our notebook. When we\u2019ve finished moving the three output_results statements to this cell, it should look like this. Now let\u2019s annotate this cell as a Pipeline Step and set the dependencies. Note that this step will depend on eval_lgbm , eval_rf , and eval_xgb because it uses the values for each score metric from each of these steps. The Depends on pull-down menu functions as a toggle. You may select as many dependencies as required. You may unselect options by clicking that item again in the pull-down menu. As depicted below, we\u2019ll select the three eval steps as dependencies for our new step, output . If you make a mistake in selecting one of the dependencies, click the incorrect dependency again in this pull-down and that item will be deselected. Once we\u2019ve selected all the dependencies, we can click away from the pull-down menu and you\u2019ll see that all three evaluation steps have been selected as dependencies.","title":"Steps w/ Multiple Dependencies"},{"location":"modules/notebook-to-pipeline/pipeline-design/steps-with-multiple-dependencies/#steps-w-multiple-dependencies","text":"In many cases, you\u2019ll need to create a step that depends on two or more other steps. Steps for producing output are one example. Let\u2019s take a look at how to set multiple dependencies by creating a final output step in our notebook. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. With the changes we\u2019ve just made to create a Functions cell containing functions for scoring and output to our notebook, each of our evaluation steps now calls output_results as its last statement. We can see this, for example in eval_xgb . Let\u2019s move the calls to output_results from each of our evaluation steps to a new step called output. Let\u2019s place the cell as the very last cell in our notebook. When we\u2019ve finished moving the three output_results statements to this cell, it should look like this. Now let\u2019s annotate this cell as a Pipeline Step and set the dependencies. Note that this step will depend on eval_lgbm , eval_rf , and eval_xgb because it uses the values for each score metric from each of these steps. The Depends on pull-down menu functions as a toggle. You may select as many dependencies as required. You may unselect options by clicking that item again in the pull-down menu. As depicted below, we\u2019ll select the three eval steps as dependencies for our new step, output . If you make a mistake in selecting one of the dependencies, click the incorrect dependency again in this pull-down and that item will be deselected. Once we\u2019ve selected all the dependencies, we can click away from the pull-down menu and you\u2019ll see that all three evaluation steps have been selected as dependencies.","title":"Steps w/ Multiple Dependencies"},{"location":"modules/notebook-to-pipeline/steps/dependencies/","text":"Dependencies Between Pipeline Steps In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. Follow Along Please follow along in your own copy of our notebook as we add a dependency for clean_data. The step clean_data relies on read_data to read our dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute. With the work we\u2019ve done so far, we now have a two-step pipeline that we can summarize as follows. Make sure you understand your data dependencies When considering how to organize your notebook into a Kubeflow pipeline it is essential that you assess the data dependencies between steps and ensure that values you\u2019ve set for the Depends on field for each step reflect these dependencies.","title":"Pipeline Step Dependencies"},{"location":"modules/notebook-to-pipeline/steps/dependencies/#dependencies-between-pipeline-steps","text":"In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. Follow Along Please follow along in your own copy of our notebook as we add a dependency for clean_data. The step clean_data relies on read_data to read our dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute. With the work we\u2019ve done so far, we now have a two-step pipeline that we can summarize as follows. Make sure you understand your data dependencies When considering how to organize your notebook into a Kubeflow pipeline it is essential that you assess the data dependencies between steps and ensure that values you\u2019ve set for the Depends on field for each step reflect these dependencies.","title":"Dependencies Between Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/","text":"Multi-Cell Pipeline Steps Text Version of this Lesson The Clean Data section of our notebook performs three different tasks to clean up our dataset. There are several cells here that, together, do the work of a data cleaning step. As we hinted in the previous section, Kale enables you to include multiple cells in a single annotation for a pipeline step. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Let\u2019s define the second step of our pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly. Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following.","title":"Multi-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/#multi-cell-pipeline-steps","text":"","title":"Multi-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/#text-version-of-this-lesson","text":"The Clean Data section of our notebook performs three different tasks to clean up our dataset. There are several cells here that, together, do the work of a data cleaning step. As we hinted in the previous section, Kale enables you to include multiple cells in a single annotation for a pipeline step. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Let\u2019s define the second step of our pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly. Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following.","title":"Text Version of this Lesson"},{"location":"modules/notebook-to-pipeline/steps/single-cell/","text":"Single-Cell Pipeline Steps Text Version of this Lesson The first step of our workflow is reading data. That section of the notebook looks like this. This line of code reads the dataset into a pandas data frame. When working with Kubeflow pipelines, it\u2019s important to be specific about exactly what code implements a pipeline step and to consider the modules on which the code depends. We also need to consider the data inputs and outputs for a given step. This step depends on the pandas module. It does not have any inputs since it is the first step of our pipeline, but it does define and set the variable df_auto , which will be used in many later steps. For purposes of Kubeflow pipelines, df_auto is an output of this pipeline step. To create our first pipeline step, we\u2019ll do a small amount of reorganizing and then apply a few annotations. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Isolate the code for your step in one cell Modify your code so that the line that reads the car_prices.csv file is in a cell by itself. Annotate the cell as a Pipeline Step and name it Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of our pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, our entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment.","title":"Single-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#single-cell-pipeline-steps","text":"","title":"Single-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#text-version-of-this-lesson","text":"The first step of our workflow is reading data. That section of the notebook looks like this. This line of code reads the dataset into a pandas data frame. When working with Kubeflow pipelines, it\u2019s important to be specific about exactly what code implements a pipeline step and to consider the modules on which the code depends. We also need to consider the data inputs and outputs for a given step. This step depends on the pandas module. It does not have any inputs since it is the first step of our pipeline, but it does define and set the variable df_auto , which will be used in many later steps. For purposes of Kubeflow pipelines, df_auto is an output of this pipeline step. To create our first pipeline step, we\u2019ll do a small amount of reorganizing and then apply a few annotations. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Text Version of this Lesson"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#isolate-the-code-for-your-step-in-one-cell","text":"Modify your code so that the line that reads the car_prices.csv file is in a cell by itself.","title":"Isolate the code for your step in one cell"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#annotate-the-cell-as-a-pipeline-step-and-name-it","text":"Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of our pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, our entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment.","title":"Annotate the cell as a Pipeline Step and name it"},{"location":"modules/rok-snapshotting/","text":"Rok 101 : Snapshotting and Restoring Kubeflow Pipelines Course Summary When deploying Kubeflow production environments, you will want to snapshot your notebooks, pipelines, and volumes so that you can rapidly restore environments or specific Kubeflow pipeline steps. We\u2019ll teach you how Rok takes snapshots, how to restore snapshots, and how to manage the volumes in the Kubernetes Pods. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We are assuming that you know how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. If you need a refresher on these skills, please review our Kale 101 course . We are also assuming that you know how to use Katib to perform hyperparameter tuning with Kubeflow Pipelines. If you need a refresher on these skills please review our Katib 101 course .","title":"Home"},{"location":"modules/rok-snapshotting/#rok-101-snapshotting-and-restoring-kubeflow-pipelines","text":"","title":"Rok 101: Snapshotting and Restoring Kubeflow Pipelines"},{"location":"modules/rok-snapshotting/#course-summary","text":"When deploying Kubeflow production environments, you will want to snapshot your notebooks, pipelines, and volumes so that you can rapidly restore environments or specific Kubeflow pipeline steps. We\u2019ll teach you how Rok takes snapshots, how to restore snapshots, and how to manage the volumes in the Kubernetes Pods. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We are assuming that you know how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. If you need a refresher on these skills, please review our Kale 101 course . We are also assuming that you know how to use Katib to perform hyperparameter tuning with Kubeflow Pipelines. If you need a refresher on these skills please review our Katib 101 course .","title":"Course Summary"},{"location":"modules/rok-snapshotting/notebook-server-base-image/","text":"Launch a Notebook Server from a Base Image To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below. 1. View the Home screen in your MiniKF Kubeflow deployment. 2. Select the Notebooks pane from the main navigation menu. You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty. 3. Click the NEW SERVER button. Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server. 4. Enter a name. In the Name field, enter a name, e.g., learn-rok-snapshotting. 5. Add a data volume. Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you. 6. Click the LAUNCH button. Scroll to the bottom of the form and click the LAUNCH button to create your notebook server. 7. Connect to your notebook server. To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"Deploy a Notebook Server"},{"location":"modules/rok-snapshotting/notebook-server-base-image/#launch-a-notebook-server-from-a-base-image","text":"To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below.","title":"Launch a Notebook Server from a Base Image"},{"location":"modules/rok-snapshotting/notebook-server-base-image/#1-view-the-home-screen-in-your-minikf-kubeflow-deployment","text":"","title":"1. View the Home screen in your MiniKF Kubeflow deployment."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#2-select-the-notebooks-pane-from-the-main-navigation-menu","text":"You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty.","title":"2. Select the Notebooks pane from the main navigation menu."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#3-click-the-new-server-button","text":"Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server.","title":"3. Click the NEW SERVER button."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#4-enter-a-name","text":"In the Name field, enter a name, e.g., learn-rok-snapshotting.","title":"4. Enter a name."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#5-add-a-data-volume","text":"Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you.","title":"5. Add a data volume."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#6-click-the-launch-button","text":"Scroll to the bottom of the form and click the LAUNCH button to create your notebook server.","title":"6. Click the LAUNCH button."},{"location":"modules/rok-snapshotting/notebook-server-base-image/#7-connect-to-your-notebook-server","text":"To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"7. Connect to your notebook server."},{"location":"modules/rok-snapshotting/rok-101-overview/","text":"Course Overview","title":"Course Overview"},{"location":"modules/rok-snapshotting/rok-101-overview/#course-overview","text":"","title":"Course Overview"},{"location":"modules/rok-snapshotting/rok-bucket-basics/","text":"Rok Bucket Basics Buckets and Snapshots To create a snapshot, Rok hashes, de-duplicates, and versions snapshots and then stores the de-duplicated, content-addressable parts on an Object Storage service (e.g., Amazon S3) that is close to the specific Kubernetes cluster. Rok creates Buckets of Snapshots which are associated with a specific set of Kubeflow pipeline runs. Snapshots are taken at the start of execution and before and after each steps, however taking snapshots does not impact application performance. Snapshotting happens outside the application I/O path using Arrikto\u2019s Linux kernel enhancements contained in the Arrikto provided Docker images. These enhancements are loaded as modules into the kernel during the creation of the Rok Cluster. As a result, snapshotting will not impact performance of applications in the Pods. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. 1. Open Snapshots In the Kubeflow UI select Snapshots to view the Rok buckets that have been created. 2. Open Bucket Expanding a bucket will show the list of snapshots that have been taken during Kubeflow pipeline execution. Each Kubeflow Pipeline run is associated with a Rok bucket. In the Snapshots UI select the newly created bucket to view the snapshots created during execution of the Kubeflow pipeline. 3. Expand a Snapshot Expand the first snapshot to see the volumes that have been captured.","title":"Rok Bucket Basics"},{"location":"modules/rok-snapshotting/rok-bucket-basics/#rok-bucket-basics","text":"","title":"Rok Bucket Basics"},{"location":"modules/rok-snapshotting/rok-bucket-basics/#buckets-and-snapshots","text":"To create a snapshot, Rok hashes, de-duplicates, and versions snapshots and then stores the de-duplicated, content-addressable parts on an Object Storage service (e.g., Amazon S3) that is close to the specific Kubernetes cluster. Rok creates Buckets of Snapshots which are associated with a specific set of Kubeflow pipeline runs. Snapshots are taken at the start of execution and before and after each steps, however taking snapshots does not impact application performance. Snapshotting happens outside the application I/O path using Arrikto\u2019s Linux kernel enhancements contained in the Arrikto provided Docker images. These enhancements are loaded as modules into the kernel during the creation of the Rok Cluster. As a result, snapshotting will not impact performance of applications in the Pods. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Buckets and Snapshots"},{"location":"modules/rok-snapshotting/rok-bucket-basics/#1-open-snapshots","text":"In the Kubeflow UI select Snapshots to view the Rok buckets that have been created.","title":"1. Open Snapshots"},{"location":"modules/rok-snapshotting/rok-bucket-basics/#2-open-bucket","text":"Expanding a bucket will show the list of snapshots that have been taken during Kubeflow pipeline execution. Each Kubeflow Pipeline run is associated with a Rok bucket. In the Snapshots UI select the newly created bucket to view the snapshots created during execution of the Kubeflow pipeline.","title":"2. Open Bucket"},{"location":"modules/rok-snapshotting/rok-bucket-basics/#3-expand-a-snapshot","text":"Expand the first snapshot to see the volumes that have been captured.","title":"3. Expand a Snapshot"},{"location":"modules/rok-snapshotting/rok-cluster-overview/","text":"Rok Cluster Overview Kubernetes Pods Kubeflow runs in Kubernetes clusters as a collection of Pods. Pods are a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. Kubeflow provides Docker images to run JupyterLab Notebooks (and other applications) in Kubernetes Pods. The NotebookServer, with Kale installed, runs as a Pod in the Kubernetes user\u2019s namespace. Arrikto makes this possible with Kale Docker Images , such as the one you selected during Notebook Server creation. During Notebook Server creation and during execution of Kubeflow pipelines facilitated by Kale the Kubernetes Pods will request storage with a PersistentVolumeClaim and will host PersistentVolumes that are provisioned in response. The Rok driver, rok-csi, prepares this volume and mounts it under the user\u2019s home directory by default. This volume hosts and stores user added code and libraries. The Rok driver (rok-csi) also provisions and mounts additional volumes, called data volumes, at locations of your choice during Notebook Server creation. Rok Cluster Rok is present on every node in a MiniKF or Enterprise Kubeflow cluster. Rok\u2019s presence enables you to create snapshots of local PersistentVolumes to capture point in time representation of volume storage. In addition to the Rok driver, the rok-operator Pod (not displayed in image) sits in the Kubernetes Cluster and serves as the orchestrator of the Rok nodes. The rok-operator communicates and synchronizes the Rok Pods across the cluster. This collection of Rok Pods with the rok-operator and rok-csi driver is often referred to as a Rok Cluster. Rok & Notebook Server During Notebook Server creation and during execution of Kubeflow pipelines facilitated by Kale, the Kubernetes Pods will request storage with a PersistentVolumeClaim and will host PersistentVolumes that are provisioned in response. The Rok driver, rok-csi, prepares and mounts the initial PersistentVolume under the user\u2019s home directory by default. The initial PersistentVolume, called the workspace volume, hosts and stores user added code and libraries. The Rok driver (rok-csi) also provisions and mounts additional volumes, called data volumes, at locations of your choice during Notebook Server creation.","title":"Rok Cluster Overview"},{"location":"modules/rok-snapshotting/rok-cluster-overview/#rok-cluster-overview","text":"","title":"Rok Cluster Overview"},{"location":"modules/rok-snapshotting/rok-cluster-overview/#kubernetes-pods","text":"Kubeflow runs in Kubernetes clusters as a collection of Pods. Pods are a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. Kubeflow provides Docker images to run JupyterLab Notebooks (and other applications) in Kubernetes Pods. The NotebookServer, with Kale installed, runs as a Pod in the Kubernetes user\u2019s namespace. Arrikto makes this possible with Kale Docker Images , such as the one you selected during Notebook Server creation. During Notebook Server creation and during execution of Kubeflow pipelines facilitated by Kale the Kubernetes Pods will request storage with a PersistentVolumeClaim and will host PersistentVolumes that are provisioned in response. The Rok driver, rok-csi, prepares this volume and mounts it under the user\u2019s home directory by default. This volume hosts and stores user added code and libraries. The Rok driver (rok-csi) also provisions and mounts additional volumes, called data volumes, at locations of your choice during Notebook Server creation.","title":"Kubernetes Pods"},{"location":"modules/rok-snapshotting/rok-cluster-overview/#rok-cluster","text":"Rok is present on every node in a MiniKF or Enterprise Kubeflow cluster. Rok\u2019s presence enables you to create snapshots of local PersistentVolumes to capture point in time representation of volume storage. In addition to the Rok driver, the rok-operator Pod (not displayed in image) sits in the Kubernetes Cluster and serves as the orchestrator of the Rok nodes. The rok-operator communicates and synchronizes the Rok Pods across the cluster. This collection of Rok Pods with the rok-operator and rok-csi driver is often referred to as a Rok Cluster.","title":"Rok Cluster"},{"location":"modules/rok-snapshotting/rok-cluster-overview/#rok-notebook-server","text":"During Notebook Server creation and during execution of Kubeflow pipelines facilitated by Kale, the Kubernetes Pods will request storage with a PersistentVolumeClaim and will host PersistentVolumes that are provisioned in response. The Rok driver, rok-csi, prepares and mounts the initial PersistentVolume under the user\u2019s home directory by default. The initial PersistentVolume, called the workspace volume, hosts and stores user added code and libraries. The Rok driver (rok-csi) also provisions and mounts additional volumes, called data volumes, at locations of your choice during Notebook Server creation.","title":"Rok &amp; Notebook Server"},{"location":"modules/rok-snapshotting/rok-overview/","text":"Rok Overview Rok is an incredibly powerful technology which addresses several enterprise MLOps and Data Science use cases. Before we begin our technical education make sure you understand all the ways that Rok can be used and all the use cases that it can address. Environment Reproducibility Much of the data being used for Machine Learning and Data Science training, testing, validation and production forecasting resides in a data lake. Unfortunately these are often cycling data and after a long period of time the data may morph or change. As a result you may not be able to come back to the environment and reproduce a specific execution data or data load. Being able to recreate an immutable snapshot is really important for a variety of industries that need this ability to \"go back in time\". For example a healthcare AI algorithm may turn down someone and an audit to confirm the model is per policy may require looking at the data that the model was trained on. If this model is several years old, then restoring all the data may be challenging, or impossible. With Rok however snapshots can quickly be loaded to reproduce the exact scenario, including the configuration of the containers and Pods, under which the model was trained. Additionally a company may have new data scientists join and others depart. Having reproducibility allows new hires to see the exact scenario under which prior data scientists did their modeling. This reproducibility is also critical for debuggability since you can quickly load the training scenario for the model and its artifacts. This reproducibility is powerful and necessary to maintain business stability. Model Portability w/ Authentication Model development can occur on a local machine, on a virtual machine or in some other type of development environment. The many different environments that model development can take place in do not have consistent or shared data storage. In fact data storage is often in silos. With Rok and Rok Registry you can move models across environments with the relevant data. You can even subscribe to the models of your colleagues or have them subscribe to yours. Rok Registry subscriptions have access controls so that management or access to models is dependent on authentication. Functionality & Performance Improvements You can take advantage of the Rok Storage Class and use it for Kubeflow to take advantage of streamlined ML workflows along with significant performance and cost infrastructure benefits. This includes easy Pod restarts across availability zones in AWS / GCP. Additionally with Kale and Rok and the Multiple Read Write volumes you can run multiple steps on multiple different nodes and distribute the whole training and workload across the nodes and can maintain dependencies across these steps and nodes. This is also backed by pipeline step caching which further improves performance and reduces cost. Additional Exploration Additional areas to explore once you complete your Rok 101 education are: Streamlined ML Pipeline building with automatic versioning Kale & Rok with hyperparameter tuning Kale & Rok automate and deploy model inference server faster Integrated Data Management for the complete DS environment. Ability to deploy a TensorBoard server from a Rok-backed artifact AutoML made easy with Kale & Rok","title":"Rok Overview"},{"location":"modules/rok-snapshotting/rok-overview/#rok-overview","text":"Rok is an incredibly powerful technology which addresses several enterprise MLOps and Data Science use cases. Before we begin our technical education make sure you understand all the ways that Rok can be used and all the use cases that it can address.","title":"Rok Overview"},{"location":"modules/rok-snapshotting/rok-overview/#environment-reproducibility","text":"Much of the data being used for Machine Learning and Data Science training, testing, validation and production forecasting resides in a data lake. Unfortunately these are often cycling data and after a long period of time the data may morph or change. As a result you may not be able to come back to the environment and reproduce a specific execution data or data load. Being able to recreate an immutable snapshot is really important for a variety of industries that need this ability to \"go back in time\". For example a healthcare AI algorithm may turn down someone and an audit to confirm the model is per policy may require looking at the data that the model was trained on. If this model is several years old, then restoring all the data may be challenging, or impossible. With Rok however snapshots can quickly be loaded to reproduce the exact scenario, including the configuration of the containers and Pods, under which the model was trained. Additionally a company may have new data scientists join and others depart. Having reproducibility allows new hires to see the exact scenario under which prior data scientists did their modeling. This reproducibility is also critical for debuggability since you can quickly load the training scenario for the model and its artifacts. This reproducibility is powerful and necessary to maintain business stability.","title":"Environment Reproducibility"},{"location":"modules/rok-snapshotting/rok-overview/#model-portability-w-authentication","text":"Model development can occur on a local machine, on a virtual machine or in some other type of development environment. The many different environments that model development can take place in do not have consistent or shared data storage. In fact data storage is often in silos. With Rok and Rok Registry you can move models across environments with the relevant data. You can even subscribe to the models of your colleagues or have them subscribe to yours. Rok Registry subscriptions have access controls so that management or access to models is dependent on authentication.","title":"Model Portability w/ Authentication"},{"location":"modules/rok-snapshotting/rok-overview/#functionality-performance-improvements","text":"You can take advantage of the Rok Storage Class and use it for Kubeflow to take advantage of streamlined ML workflows along with significant performance and cost infrastructure benefits. This includes easy Pod restarts across availability zones in AWS / GCP. Additionally with Kale and Rok and the Multiple Read Write volumes you can run multiple steps on multiple different nodes and distribute the whole training and workload across the nodes and can maintain dependencies across these steps and nodes. This is also backed by pipeline step caching which further improves performance and reduces cost.","title":"Functionality &amp; Performance Improvements"},{"location":"modules/rok-snapshotting/rok-overview/#additional-exploration","text":"Additional areas to explore once you complete your Rok 101 education are: Streamlined ML Pipeline building with automatic versioning Kale & Rok with hyperparameter tuning Kale & Rok automate and deploy model inference server faster Integrated Data Management for the complete DS environment. Ability to deploy a TensorBoard server from a Rok-backed artifact AutoML made easy with Kale & Rok","title":"Additional Exploration"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/","text":"Rok Snapshot Basics Kale generates Kubeflow pipelines from tagged cells in the JupyterLab Notebook, which has a default workspace volume and one or more default data volumes. Arrikto does recommend defining at least one data volume during Notebook Server creation. When a Kubeflow pipeline is instantiated, Rok will snapshot all Notebook volumes and then clone them for the pipeline steps to use them. By seamlessly cloning the workspace and data volumes, your environment is versioned and replicated to the new pipeline which is then executed. Initial Rok Snapshots To view the Rok Snapshot steps open the Kubeflow pipeline generated by the Katib experiment. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. 1. Run Kubeflow Pipeline To begin execute the Kubeflow pipeline generated by the Kale tags already present in the Notebook by selecting COMPILE AND RUN . 2. Review Initial Snapshot To view the details of the initial Rok snapshot select the Done option next to the Taking snapshot step of the Kubeflow pipeline. The snapshot details will be displayed on screen. 3. Review Snapshot Metadata Scroll down to review the snapshot metadata. 4. Review Snapshot Steps Return to the Notebook and select the View option next to the Running pipeline step of the Kubeflow pipeline. In the Kubeflow pipeline you will see a step called create-volume-1 , this is the step responsible for provisioning a clone of the Notebook\u2019s workspace volume. If there are data volumes attached to the Notebook then you will see one step per data volume labelled as create-volume-# . Kale Marshal Volumes There are two important reasons for this duplication process. Marshalling: a mechanism to seamlessly pass data between steps. Kale uses a hidden folder within the workspace volume as the shared marshalling location to serialize and deserialize data. Reproducibility, experimentation: when working on a Notebook, it is often the case that you install new libraries, write new modules, create or download assets required by your code. By seamlessly cloning the workspace and data volumes, all your environment is versioned and replicated to the new pipeline. The newly duplicated volumes are mounted by the rok-csi driver to the container in which the pipeline steps run. You can select the pipeline step cell and select volumes to see where the dependent volumes are mounted. Pipeline Step Snapshots Rok takes snapshots just before and just after execution of individual pipeline steps. This functionality allows you to restore not just an environment but the execution state of a Kubeflow pipeline initiated by Kale from within a JupyterLab Notebook. 5. Review Step Snapshot Select the read_data Kubeflow pipeline step. Then select the Visualizations option on the right to see the snapshot acknowlegement. 6. Review Snapshot Details Select the snapshot in Rok UI link to view the snapshot details. The snapshot details will open in the browser.","title":"Rok Snapshot Basics"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#rok-snapshot-basics","text":"Kale generates Kubeflow pipelines from tagged cells in the JupyterLab Notebook, which has a default workspace volume and one or more default data volumes. Arrikto does recommend defining at least one data volume during Notebook Server creation. When a Kubeflow pipeline is instantiated, Rok will snapshot all Notebook volumes and then clone them for the pipeline steps to use them. By seamlessly cloning the workspace and data volumes, your environment is versioned and replicated to the new pipeline which is then executed.","title":"Rok Snapshot Basics"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#initial-rok-snapshots","text":"To view the Rok Snapshot steps open the Kubeflow pipeline generated by the Katib experiment. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Initial Rok Snapshots"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#1-run-kubeflow-pipeline","text":"To begin execute the Kubeflow pipeline generated by the Kale tags already present in the Notebook by selecting COMPILE AND RUN .","title":"1. Run Kubeflow Pipeline"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#2-review-initial-snapshot","text":"To view the details of the initial Rok snapshot select the Done option next to the Taking snapshot step of the Kubeflow pipeline. The snapshot details will be displayed on screen.","title":"2. Review Initial Snapshot"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#3-review-snapshot-metadata","text":"Scroll down to review the snapshot metadata.","title":"3. Review Snapshot Metadata"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#4-review-snapshot-steps","text":"Return to the Notebook and select the View option next to the Running pipeline step of the Kubeflow pipeline. In the Kubeflow pipeline you will see a step called create-volume-1 , this is the step responsible for provisioning a clone of the Notebook\u2019s workspace volume. If there are data volumes attached to the Notebook then you will see one step per data volume labelled as create-volume-# .","title":"4. Review Snapshot Steps"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#kale-marshal-volumes","text":"There are two important reasons for this duplication process. Marshalling: a mechanism to seamlessly pass data between steps. Kale uses a hidden folder within the workspace volume as the shared marshalling location to serialize and deserialize data. Reproducibility, experimentation: when working on a Notebook, it is often the case that you install new libraries, write new modules, create or download assets required by your code. By seamlessly cloning the workspace and data volumes, all your environment is versioned and replicated to the new pipeline. The newly duplicated volumes are mounted by the rok-csi driver to the container in which the pipeline steps run. You can select the pipeline step cell and select volumes to see where the dependent volumes are mounted.","title":"Kale Marshal Volumes"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#pipeline-step-snapshots","text":"Rok takes snapshots just before and just after execution of individual pipeline steps. This functionality allows you to restore not just an environment but the execution state of a Kubeflow pipeline initiated by Kale from within a JupyterLab Notebook.","title":"Pipeline Step Snapshots"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#5-review-step-snapshot","text":"Select the read_data Kubeflow pipeline step. Then select the Visualizations option on the right to see the snapshot acknowlegement.","title":"5. Review Step Snapshot"},{"location":"modules/rok-snapshotting/rok-snapshot-basics/#6-review-snapshot-details","text":"Select the snapshot in Rok UI link to view the snapshot details. The snapshot details will open in the browser.","title":"6. Review Snapshot Details"},{"location":"modules/rok-snapshotting/rok-snapshot-restoration/","text":"Rok Snapshot Restoration Rok snapshots provide the ability to version entire environments much in the same way that you can version code with Git. Rok snapshots enable you to iterate on your environment as you transform datasets, develop models, and develop other aspects of an ML workflow. The pipeline is always reproducible, thanks to the immutable snapshots, and you do not have to build new Docker images for each pipeline run. Rok Group Snapshot URL Rok enables restoring group snapshots in their entirety on new Notebook Servers. This enables you to recreate a complete environment exactly the way it was at any point in time. To do this you need to copy the URL from the Rok Snapshot. With the URL copied you can provide the Rok Group Snapshot as reference when creating a new Notebook Server. If the URL is accepted you will see a notification as shown in the image below: Rok Volume Snapshot URL Rok permits loading specific workspace or data volumes snapshots during creation of the Notebook Server. As with the entire group snapshot each of the volumes snapshots (workspace and data) has a specific URL with the workspace volume always first in the list of snapshot volumes. You can then specify this URL for either workspace or data volume snapshot by selecting Existing and specifying the URL when creating the Notebook Server and adding volumes to it. Rok Pipeline Step Snapshot URL Rok also permits loading the group snapshot taken before or after the execution of pipeline steps into a Notebook Server to recreate the captured execution state. The snapshot and the associated URL can be accessed from the \u201csnapshot in the Rok UI\u201d link under the Visualizations option in the Kubeflow UI. Once the group snapshot is open you can copy the URL by selecting the copy icon.","title":"Rok Snapshot Restoration"},{"location":"modules/rok-snapshotting/rok-snapshot-restoration/#rok-snapshot-restoration","text":"Rok snapshots provide the ability to version entire environments much in the same way that you can version code with Git. Rok snapshots enable you to iterate on your environment as you transform datasets, develop models, and develop other aspects of an ML workflow. The pipeline is always reproducible, thanks to the immutable snapshots, and you do not have to build new Docker images for each pipeline run.","title":"Rok Snapshot Restoration"},{"location":"modules/rok-snapshotting/rok-snapshot-restoration/#rok-group-snapshot-url","text":"Rok enables restoring group snapshots in their entirety on new Notebook Servers. This enables you to recreate a complete environment exactly the way it was at any point in time. To do this you need to copy the URL from the Rok Snapshot. With the URL copied you can provide the Rok Group Snapshot as reference when creating a new Notebook Server. If the URL is accepted you will see a notification as shown in the image below:","title":"Rok Group Snapshot URL"},{"location":"modules/rok-snapshotting/rok-snapshot-restoration/#rok-volume-snapshot-url","text":"Rok permits loading specific workspace or data volumes snapshots during creation of the Notebook Server. As with the entire group snapshot each of the volumes snapshots (workspace and data) has a specific URL with the workspace volume always first in the list of snapshot volumes. You can then specify this URL for either workspace or data volume snapshot by selecting Existing and specifying the URL when creating the Notebook Server and adding volumes to it.","title":"Rok Volume Snapshot URL"},{"location":"modules/rok-snapshotting/rok-snapshot-restoration/#rok-pipeline-step-snapshot-url","text":"Rok also permits loading the group snapshot taken before or after the execution of pipeline steps into a Notebook Server to recreate the captured execution state. The snapshot and the associated URL can be accessed from the \u201csnapshot in the Rok UI\u201d link under the Visualizations option in the Kubeflow UI. Once the group snapshot is open you can copy the URL by selecting the copy icon.","title":"Rok Pipeline Step Snapshot URL"},{"location":"modules/rok-snapshotting/upload-handouts/","text":"Get Dataset and Code To work through this module you will need the code and data we have provided. Please download and unzip the handout . Upload the handout files Once you\u2019ve unzipped the handout, you should see the following files. 1. Review the handout files car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step. 2. Open the learn-rok-snapshotting-vol-1 folder Double-click on the directory, learn-rok-snapshotting-vol-1 . Note that this is the name of the data volume you added to your notebook server previously. If you named your notebook differently, then you will be seeing a different folder name here. 3. Click the file upload button 4. Upload handout files In the file dialog that pops up, select the handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-rok-snapshotting-vol-1 directory. 5. Create a new folder Click the button to create a new folder. 6. Name the folder \"data\" 7. Move data files Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder. 8. Open our notebook Double-click predict_car_price_rok.ipynb in the file browser pane. 9. Enable Kale Click the Enable toggle in the Kale Deployment panel to enable Kale. 10. Launch a Terminal Click the Launcher tab and launch a Terminal . 11. Install Requirements In the terminal enter the following commands. Change to the learn-rok-snapshotting-vol-1 directory. cd learn-rok-snapshotting-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt 12. Restart the Kernel In the predict_car_price.ipynb notebook, restart the kernel.","title":"Get Dataset and Code"},{"location":"modules/rok-snapshotting/upload-handouts/#get-dataset-and-code","text":"To work through this module you will need the code and data we have provided. Please download and unzip the handout .","title":"Get Dataset and Code"},{"location":"modules/rok-snapshotting/upload-handouts/#upload-the-handout-files","text":"Once you\u2019ve unzipped the handout, you should see the following files.","title":"Upload the handout files"},{"location":"modules/rok-snapshotting/upload-handouts/#1-review-the-handout-files","text":"car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step.","title":"1. Review the handout files"},{"location":"modules/rok-snapshotting/upload-handouts/#2-open-the-learn-rok-snapshotting-vol-1-folder","text":"Double-click on the directory, learn-rok-snapshotting-vol-1 . Note that this is the name of the data volume you added to your notebook server previously. If you named your notebook differently, then you will be seeing a different folder name here.","title":"2. Open the learn-rok-snapshotting-vol-1 folder"},{"location":"modules/rok-snapshotting/upload-handouts/#3-click-the-file-upload-button","text":"","title":"3. Click the file upload button"},{"location":"modules/rok-snapshotting/upload-handouts/#4-upload-handout-files","text":"In the file dialog that pops up, select the handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-rok-snapshotting-vol-1 directory.","title":"4. Upload handout files"},{"location":"modules/rok-snapshotting/upload-handouts/#5-create-a-new-folder","text":"Click the button to create a new folder.","title":"5. Create a new folder"},{"location":"modules/rok-snapshotting/upload-handouts/#6-name-the-folder-data","text":"","title":"6. Name the folder \"data\""},{"location":"modules/rok-snapshotting/upload-handouts/#7-move-data-files","text":"Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder.","title":"7. Move data files"},{"location":"modules/rok-snapshotting/upload-handouts/#8-open-our-notebook","text":"Double-click predict_car_price_rok.ipynb in the file browser pane.","title":"8. Open our notebook"},{"location":"modules/rok-snapshotting/upload-handouts/#9-enable-kale","text":"Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"9. Enable Kale"},{"location":"modules/rok-snapshotting/upload-handouts/#10-launch-a-terminal","text":"Click the Launcher tab and launch a Terminal .","title":"10. Launch a Terminal"},{"location":"modules/rok-snapshotting/upload-handouts/#11-install-requirements","text":"In the terminal enter the following commands. Change to the learn-rok-snapshotting-vol-1 directory. cd learn-rok-snapshotting-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt","title":"11. Install Requirements"},{"location":"modules/rok-snapshotting/upload-handouts/#12-restart-the-kernel","text":"In the predict_car_price.ipynb notebook, restart the kernel.","title":"12. Restart the Kernel"},{"location":"modules/rok-snapshotting/labs/lab-initial-volume-mounts/","text":"Lab: Initial Volume Mounts Before the Kubeflow Pipeline run starts, Rok takes a snapshot of the workspace and data volume. Then, it clones these snapshots to create new volumes that the pipeline steps will use. The initial clones of the workspace volume and data volume are mounted to each of the containers that are created to process the Kubeflow pipeline steps. Requirements You can identify where these volumes are mounted by reviewing the pipeline steps. Review the pipeline step clean_data to identify where the volumes are mounted. Solution View Solution The volumes will be mounted to the /home/jovyan directory, with sub directories created for the data volumes.","title":"Lab: Initial Volume Mounts"},{"location":"modules/rok-snapshotting/labs/lab-initial-volume-mounts/#lab-initial-volume-mounts","text":"Before the Kubeflow Pipeline run starts, Rok takes a snapshot of the workspace and data volume. Then, it clones these snapshots to create new volumes that the pipeline steps will use. The initial clones of the workspace volume and data volume are mounted to each of the containers that are created to process the Kubeflow pipeline steps.","title":"Lab: Initial Volume Mounts"},{"location":"modules/rok-snapshotting/labs/lab-initial-volume-mounts/#requirements","text":"You can identify where these volumes are mounted by reviewing the pipeline steps. Review the pipeline step clean_data to identify where the volumes are mounted.","title":"Requirements"},{"location":"modules/rok-snapshotting/labs/lab-initial-volume-mounts/#solution","text":"View Solution The volumes will be mounted to the /home/jovyan directory, with sub directories created for the data volumes.","title":"Solution"},{"location":"modules/rok-snapshotting/labs/lab-rok-snapshot-restoration/","text":"Lab: Rok Snapshot Restoration Rok Snapshots Sets are stored in a Rok Bucket which can be referenced and restored during creation of a Notebook Server. Requirements Create a new Notebook Server using the Rok Snapshot Set that was created during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook. Hint View Hint In the Rok Bucket you need to copy the Snapshot URL. In the Notebook Server Creation UI you need to paste the Rok URL. Solution View Solution First you must copy the snapshot set URL. Second you need to paste the URL when creating the Notebook Server. Third confirm the load by navigating to the Notebook.","title":"Lab: Rok Snapshot Restoration"},{"location":"modules/rok-snapshotting/labs/lab-rok-snapshot-restoration/#lab-rok-snapshot-restoration","text":"Rok Snapshots Sets are stored in a Rok Bucket which can be referenced and restored during creation of a Notebook Server.","title":"Lab: Rok Snapshot Restoration"},{"location":"modules/rok-snapshotting/labs/lab-rok-snapshot-restoration/#requirements","text":"Create a new Notebook Server using the Rok Snapshot Set that was created during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook.","title":"Requirements"},{"location":"modules/rok-snapshotting/labs/lab-rok-snapshot-restoration/#hint","text":"View Hint In the Rok Bucket you need to copy the Snapshot URL. In the Notebook Server Creation UI you need to paste the Rok URL.","title":"Hint"},{"location":"modules/rok-snapshotting/labs/lab-rok-snapshot-restoration/#solution","text":"View Solution First you must copy the snapshot set URL. Second you need to paste the URL when creating the Notebook Server. Third confirm the load by navigating to the Notebook.","title":"Solution"},{"location":"modules/rok-snapshotting/labs/lab-snapshot-volume-restoration/","text":"Lab: Volume Snapshot Restoration Rok Snapshots Sets are stored in a Rok Bucket and the specific workspace or data volume snapshot can be referenced and restored during creation of a Notebook Server. Requirements Create a new Notebook Server using the workspace and data volume snapshot contained in the Rok Snapshot Set that was created during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook. Hint View Hint In the Rok Bucket you need to copy the Volume URL. In the Notebook Server Creation UI you need to paste the Rok URL for each volume. Solution View Solution First you must copy each volume URL. Second you need to change the Workspace and Data Volume types to Existing and paste the respective URL into the Rok URL text box when creating the Notebook Server. You must not fill in the Rok URL at the Notebook Server level only fill in the volumes. Third confirm the load by navigating to the Notebook.","title":"Lab: Snapshot Volume Resotration"},{"location":"modules/rok-snapshotting/labs/lab-snapshot-volume-restoration/#lab-volume-snapshot-restoration","text":"Rok Snapshots Sets are stored in a Rok Bucket and the specific workspace or data volume snapshot can be referenced and restored during creation of a Notebook Server.","title":"Lab: Volume Snapshot Restoration"},{"location":"modules/rok-snapshotting/labs/lab-snapshot-volume-restoration/#requirements","text":"Create a new Notebook Server using the workspace and data volume snapshot contained in the Rok Snapshot Set that was created during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook.","title":"Requirements"},{"location":"modules/rok-snapshotting/labs/lab-snapshot-volume-restoration/#hint","text":"View Hint In the Rok Bucket you need to copy the Volume URL. In the Notebook Server Creation UI you need to paste the Rok URL for each volume.","title":"Hint"},{"location":"modules/rok-snapshotting/labs/lab-snapshot-volume-restoration/#solution","text":"View Solution First you must copy each volume URL. Second you need to change the Workspace and Data Volume types to Existing and paste the respective URL into the Rok URL text box when creating the Notebook Server. You must not fill in the Rok URL at the Notebook Server level only fill in the volumes. Third confirm the load by navigating to the Notebook.","title":"Solution"},{"location":"modules/rok-snapshotting/labs/lab-step-snapshot-restoration/","text":"Lab: Step Snapshot Restoration Rok takes snapshots before and after each step in the Kubeflow pipeline. Rok Group Snapshots for pipeline steps can be referenced and restored during the creation of a Notebook Server. This will restore the exact state of processing into the Notebook Server memory for the Notebook that is created with the snapshot. This way you can recreate the exact moment that the snapshot was taken during processing. Requirements Create a new Notebook Server using the Rok Group Snapshot for the clean_data step from the Kubeflow pipeline execution that was triggered during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook. Hint View Hint In the Runs UI you need to select the run created during Rok Snapshot Basics and then select the clean_data step to access the Rok Snapshot. In the Notebook Server Creation UI you need to paste the Rok URL. Solution View Solution First you must open the run from the Runs UI. Second you need select the run that was generated during Rok Snapshot Basics . Third select the clean_data step and select the snapshot in the Rok UI link from Visualizations . Fourth copy the URL for the snapshot. Fifth paste the URL into the Rok JupyterLab URL box when creating a new Notebook Server. Now confirm that the restoration was successful by opening the Notebook - you will see a message that the Notebook is resuming at the clean_data step.","title":"Lab: Pipeline Step Restoration"},{"location":"modules/rok-snapshotting/labs/lab-step-snapshot-restoration/#lab-step-snapshot-restoration","text":"Rok takes snapshots before and after each step in the Kubeflow pipeline. Rok Group Snapshots for pipeline steps can be referenced and restored during the creation of a Notebook Server. This will restore the exact state of processing into the Notebook Server memory for the Notebook that is created with the snapshot. This way you can recreate the exact moment that the snapshot was taken during processing.","title":"Lab: Step Snapshot Restoration"},{"location":"modules/rok-snapshotting/labs/lab-step-snapshot-restoration/#requirements","text":"Create a new Notebook Server using the Rok Group Snapshot for the clean_data step from the Kubeflow pipeline execution that was triggered during the Rok Snapshot Basics section. Confirm successful restoration by opening the Notebook.","title":"Requirements"},{"location":"modules/rok-snapshotting/labs/lab-step-snapshot-restoration/#hint","text":"View Hint In the Runs UI you need to select the run created during Rok Snapshot Basics and then select the clean_data step to access the Rok Snapshot. In the Notebook Server Creation UI you need to paste the Rok URL.","title":"Hint"},{"location":"modules/rok-snapshotting/labs/lab-step-snapshot-restoration/#solution","text":"View Solution First you must open the run from the Runs UI. Second you need select the run that was generated during Rok Snapshot Basics . Third select the clean_data step and select the snapshot in the Rok UI link from Visualizations . Fourth copy the URL for the snapshot. Fifth paste the URL into the Rok JupyterLab URL box when creating a new Notebook Server. Now confirm that the restoration was successful by opening the Notebook - you will see a message that the Notebook is resuming at the clean_data step.","title":"Solution"}]}